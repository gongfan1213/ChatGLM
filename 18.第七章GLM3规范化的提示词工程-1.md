

### 第7章 构建以人为本的ChatGLM3规范化Prompt提示工程
在探索编程模型的新领域时，应始终坚守“以人为本”的设计理念，这也是贯彻人工智能模型服务于人类的核心思想。构建以人为本的提示工程，其实质是为了更好地理解和适应人类的需求与习惯。这些作为模型输入的提示，不再是冰冷、刻板的硬编码，而是由多个灵活多变的组件根据人们的实际需求精心构建而成的。

ChatGLM3作为这一创新理念的执行者，致力于构建出更加人性化、智能化的输入方式。它像是一位贴心的助手，时刻关注着用户的需求，为用户提供更便捷、更高效的解决方案。它提供的各种类和函数，均围绕着如何更好地服务于人类而设计。无论是构建提示、处理输出，还是优化用户体验，ChatGLM3都始终将人的需求放在首位。

本章将探讨以人为本的提示模板的构建方法，并结合ChatGLM3终端演示提示模板的用法。

#### 7.1 提示工程模板构建的输入与输出格式
在处理复杂的聊天交互时，大语言模型展现出了其独特的优势。它们能接收简单的聊天消息列表作为输入，这些列表在行业内常被称为“提示”。更重要的是，每一条消息都被赋予了特定的角色属性。这种角色化的处理方式，使得聊天内容不再是单调的字符串堆砌，而是富含上下文信息和交互逻辑的沟通流。

在接下来的内容中，将深入剖析提示工程模板的构建精髓，细致探讨其输入与输出的标准格式。我们要求大型模型在回答问题时必须严格遵守既定的规范，这不仅确保了回答的一致性和准确性，更有助于我们充分发挥大型模型的潜能，实现更高效、更精准的交互体验。

##### 7.1.1 提示模板的输入格式
智谱AI的ChatGLM3允许将聊天消息与AI、人类或系统角色紧密关联。这种关联性不仅提升了交流的清晰度，还能指导模型更加精确地遵循系统发出的指令。因此，在构建聊天应用时，对系统消息的响应准确性和及时性成为衡量模型性能的重要指标。

正是基于这样的背景，我们可以设计一系列与聊天紧密相关的提示模板。这些模板旨在帮助开发者更轻松地构建和处理复杂的聊天提示，从而充分发挥底层聊天模型的潜力。当需要查询或交互聊天模型时，使用这些专为聊天设计的提示模板，而非通用的PromptTemplate，将能够带来更加流畅、自然的用户体验。

在与大语言模型进行交互问答时，一般涉及的提示模板分为以下几类：
- ChatPromptTemplate：这是一个基类或模板类，用于构建和处理与聊天相关的提示。它可能包含一些通用的方法和属性，用于格式化、解析和传递聊天提示。
- PromptTemplate：这是一个更通用的提示模板类，可用于构建各种类型的提示，而不仅仅是聊天提示。它可能提供了一些基本的方法来构建和修改提示。
- SystemMessagePromptTemplate：这个类专用于构建整体系统描述的提示。
- AIMessagePromptTemplate：这个类专用于构建和处理由AI生成的消息的提示。AI消息是由聊天模型或其他AI算法生成的响应，用于回答用户的问题或提供信息。
- HumanMessagePromptTemplate：这个类专用于构建和处理由人类用户生成的消息的提示。人类消息是用户在聊天过程中输入的文字或其他形式的数据。

在具体使用上，可以使用LangChain中的现成模块进行导入：
```python
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
```
下面示例采用格式化方法完成提示模板的构建，代码如下：
```python
# 首先导入上面的代码
template="You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

for prompt in chat_prompt:
    print(prompt)
```
打印chat_prompt内容如图7 - 1所示。可以看到，此时的结构化内容依次通过字典的形式进行了分类，输入与输出内容都被放置在input_variables参数中，message则是构建的整体描述字典。
（此处图7 - 1内容因无实际图片无法准确呈现，原文为相关字典结构展示）

![image](https://github.com/user-attachments/assets/84b463c9-4d34-4126-b5ee-09b556286d7c)


下面需要将输入的对话模板构建成一个完整的提示内容，读者可以使用chat_prompt中的format函数或者format_prompt函数来构建一个完整的对话内容，代码如下：
```python
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

template="You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# 通过格式化的方法完成提示模板的构建
output = chat_prompt.format(input_language="English", output_language="chinese", text="I love ChatGLM.")
print(output)
message = chat_prompt.format_prompt(input_language="English", output_language="chinese", text="I love ChatGLM.").to_messages()
print(message)
```

![image](https://github.com/user-attachments/assets/455562c2-1a17-45f4-b614-534e53fbcee8)


打印结果如图7 - 2所示。

（此处图7 - 2内容因无实际图片无法准确呈现，原文为构建的提示模板内容展示）

可以看到，此时的模板根据输入调用的内容被整合成一个完整的提示语句，而message则担负传送消息的任务。

除了前面介绍的内容之外，SystemMessagePromptTemplate、AIMessagePromptTemplate、HumanMessagePromptTemplate作为不同角色的模版，用于创建不同角色的提示信息内容。在具体使用上，它们与之前介绍的提示模板用法一致，送读者自行尝试。

但是，在聊天模型支持使用任意角色发送聊天消息的情况下，还有一种方法可以使用自定义的角色，即使用ChatMessagePromptTemplate允许用户指定角色名称。
```python
from langchain.prompts import ChatMessagePromptTemplate
prompt = "May the {subject} be with you"
chat_message_prompt = ChatMessagePromptTemplate.from_template(role="chatGLM", template=prompt)
chat_message_prompt.format(subject="force")
```
上面代码直接对提示模板做了定义，role显式地表明此条信息来自ChatGLM，这也给我们在后期更自由地定义提示模板做了铺垫。

##### 7.1.2 提示模板的输出格式
对于提示模板的输出格式，我们仍然要求大模型在输出时遵循特定的格式，即同样要求做到“以人为本，需求第一”。下面将讲解提示模板的输出格式，首先来看一个简单的示例：
```python
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="List five {subject}.\n{format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions}
)

_input = prompt.format(subject="ice cream flavors")
print(_input)
```
在上面代码中，CommaSeparatedListOutputParser对象是个解析器，可以处理以逗号分隔的列表输出，将其转换为Python列表。调用get_format_instructions()方法，从输出解析器中获取格式化指令，这些指令通常用于指导语言模型如何格式化其输出，以便之后可以被该解析器正确解析。使用PromptTemplate类创建了一个提示模板，这个模板包含两个变量部分——{subject}和{format_instructions}，其中参数说明如下：
- template参数定义了模板的文本。在这里，它要求列出5个特定的主题，并附加了格式化指令。
- input_variables参数是一个列表，定义了哪些变量需要在格式化时提供。在这里，只需要提供subject。
- partial_variables参数是一个字典，定义了模板中已经部分填充的变量。在这里，format_instructions已经被赋予了一个值。

可以看到这段代码的目的是创建一个提示，用于请求列出5个特定的主题（在这个例子中是“ice cream flavors” ），并确保输出是一种可以被CommaSeparatedListOutputParser解析的格式。

我们还可以将提示与ChatGLM3终端相连接，从而根据终端的输出完成对提示的回答，代码如下：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

response = llm(_input)
print(response)
```
输出结果请读者自行打印，并与上一节的示例代码相互比较。

在某些情况下，我们还需要将输出的结果作为结构化的抽取，并将其输出。下面定义一种要求大模型进行结构化输出的提示模板：
```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate

response_schemas = [
    ResponseSchema(name="answer", description="answer to the user's question"),
    ResponseSchema(name="source", description="source used to answer the user's question, should be a website.")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="answer the users question as best as possible.\n{format_instructions}\n{question}",
    input_variables=["question"],
    partial_variables={"format_instructions": format_instructions}
)

from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

_input = prompt.format_prompt(question="what's the capital of france")
response = llm(_input.to_string())
print(response)
```

运行代码后，我们将获得一个既包含回复内容又包含回复来源的文本答案，如图7 - 3所示。

（此处图7 - 3内容因无实际图片无法准确呈现，原文为“The capital of France is Paris. Source: <https://www.website>” ）

![image](https://github.com/user-attachments/assets/d91725dd-c1d4-433a-800e-c466f1367354)


下面我们对代码进行分析。

(1) 定义输出的模版样式：
```python
response_schemas = [
    ResponseSchema(name="answer", description="answer to the user's question"),
    ResponseSchema(name="source", description="source used to answer the user's question, should be a website.")
]
```

这里定义了两个ResponseSchema对象，它们描述了期望的输出结构。第一个模式名为“answer”，用于存储对用户提出的问题的答案；第二个模式名为“source”，用于指明用来回答问题的来源网站。

(2) 创建输出解析器：
```python
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
```
这个输出解析器的作用是使用之前定义的响应模式列表response_schemas来创建一个StructuredOutputParser对象。这个解析器能够解析与这些模式匹配的结构化输出。

(3) 获取格式化指令：
```python
format_instructions = output_parser.get_format_instructions()
```
从输出解析器中获取格式化指令，这些指令将指导语言模型如何格式化其输出，以便之后该解析器能够正确解析。

(4) 创建对话模板：
```python
prompt = PromptTemplate(
    template="answer the users question as best as possible.\n{format_instructions}\n{question}",
    input_variables=["question"],
    partial_variables={"format_instructions": format_instructions}
)
```
这是创建一个对话模板，用于指导语言模型如何回答用户的问题。其中包含了3个部分：

- 固定的文本：“answer the users question as best as possible.” 。

- 格式化指令: {format_instructions}，它将被之前从输出解析器中获取的格式化指令替换。

- 用户的问题: {question}，这是一个占位符，稍后在格式化提示时将用实际的用户问题替换。


在具体使用该模板时，需要提供一个question变量来格式化提示。例如：

![image](https://github.com/user-attachments/assets/82998140-c01f-492f-881a-05e44f3b0617)


```python
_input = prompt.format(question="What is the capital of France?")
print(_input)
```
这将输出一个格式化的提示，类似于：
answer the users question as best as possible.
[具体的格式化指令]
What is the capital of France?

这个格式化的提示可以进一步用于查询语言模型，以获取结构化的回答。

最后使用基于ChatGLM3的终端完成对结果的输出，具体请读者自行查看。

![image](https://github.com/user-attachments/assets/8780d0ca-d3db-43e2-b447-1fc26d5828ee)


#### 7.2 提示工程模板高级用法

在之前的讨论中，我们已经概述了提示工程模板构建的基础方法。在这个构建过程中，有几个核心要点值得特别关注。

1. **提升提示模板的人性化设计**

为了使语言模型能更深入地理解和回应人类的需求，需要深入研究如何在PromptTemplates中融入更多人性化的元素。这意味着模型不仅需要理解语言的字面意思，还要能够捕捉到语言背后的情感、语境和文化背景，从而以更接近人类思维和表达习惯的方式来做出回应。

2. **优化聊天提示模板的交互体验**

在聊天模型的构建中，用户的交互体验是我们关注的另一个重点。笔者的建议是致力于让每一次对话都变得轻松自然，仿佛是与一个了解你、关心你的好友在交流。为实现这一目标，需要不断优化模型的响应速度、对话的连贯性，以及对需求意图的准确理解。

3. **以用户需求为导向的输出格式**

对于大模型生成的回复，需要更加注重输出格式的用户需求导向性。这意味着输出不仅要提供清晰、结构化的信息，还要能够根据用户的实时反馈和需求，进行灵活的调整和优化。一个基本目标是让每一位用户都能从模型中获得最符合自己需求、最实用且最满意的输出结果，从而能够进一步提升语言模型的实用性和用户满意度。

##### 7.2.1 提示模板的自定义格式
假设希望大型语言模型（LLM终端）能够根据函数的名称自动生成对应的英语解释。为了实现这一目标，我们将设计一个专门的自定义提示模板。这个模板将接收函数名称作为输入，并以其为基础，格式化并提供相关的函数源代码作为引导信息。那么，为什么需要自定义提示模板呢？

虽然默认的提示模板在多种任务中都能生成有效的提示，但是在实际应用中，我们可能会遇到默认模板无法满足特定需求的情况。例如，有时我们可能需要为语言模型提供更具体、更动态的指示信息。

在这种情况下，自定义提示模板就显得尤为重要。通过创建自定义模板，我们可以精确地控制提示的内容、格式和风格，从而确保语言模型能够按照我们的期望生成准确、相关且有用的输出。这不仅提高了模型的灵活性和适应性，还为我们提供了更广阔的创作空间，使我们能够根据实际需求定制符合要求的提示。

如下所示的代码是一个完整的LLM终端，它能够对自定义函数进行源码解析。
```python
import inspect
def get_source_code(function_name):
    # Get the source code of the function
    return inspect.getsource(function_name)

def add_fun(a,b):
    return a + b

function_name = add_fun
source_code = get_source_code(function_name)

prompt = f"Given the function name and source code, generate an English language explanation of the function. Function Name: {function_name} Source Code: {source_code} Explanation,用中文回复。: "
print(prompt)

from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

response = llm(prompt)
print(response)
```
请读者自行运行并查看结果。

##### 7.2.2 提示模板的FewShotPromptTemplate格式
在4.1.5节中，笔者利用模板完成了反义词的生成工作，相较于仅依赖语言对需求的笼统描述，那里巧妙地运用了少量具体的使用需求示例。这些示例结合ChatGLM3强大的推理能力，显著提升了用户体验，使用户能够更为便捷地生成所需结果。

值得一提的是，FewShotPromptTemplate在此过程中发挥了核心作用。作为常用的示例提示模板，FewShotPromptTemplate能够高效地将少量用户自定义数据传递给LLM终端，进而明确指导模型的解读和生成结果方向，确保输出的准确性和贴合度。一个简单的格式使用示例代码如下：
```python
# 导入langchain库中的FewShotPromptTemplate和PromptTemplate
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

# 创建示例对话
examples = [
    {
        "query": "How are you?",  # 用户查询
        "answer": "I am fine."  # AI回答
    },
    {
        "query": "现在几点了?",  # 用户查询
        "answer": "该吃晚饭了"  # AI回答
    }
]

# 创建一个示例模板
example_template = """
User: {query}  # 用户输入部分
AI: {answer}  # AI响应部分
"""

# 使用上面的模板创建一个PromptTemplate对象
example_prompt = PromptTemplate(
    input_variables=["query", "answer"],  # 模板中的变量
    template=example_template  # 模板内容
)

# 将之前的提示拆分为前缀和后缀
# 前缀是我们的指令
prefix = """你是一个风趣幽默的机器人，可以用有趣的语调回答用户的提问
"""
# 后缀是用户输入和输出指示符
suffix = """
User: {query}  # 用户
```

### 7.2.2 提示模板的FewShotPromptTemplate格式（续）
```
输入部分
AI: """  # AI响应部分（注意这里没有{answer}，因为在实际使用时，AI的响应是未知的）

# 创建一个FewShotPromptTemplate对象
few_shot_prompt_template = FewShotPromptTemplate(
    examples=examples,  # 示例对话
    example_prompt=example_prompt,  # 示例模板
    prefix=prefix,  # 前缀
    suffix=suffix,  # 后缀
    input_variables=["query"],  # 输入变量（在这里只有用户的查询）
    example_separator="\n\n"  # 示例之间的分隔符
)

# 设置一个用户查询
query = "今天下雨了吗？"
# 使用FewShotPromptTemplate对象格式化用户查询，并打印结果
print(few_shot_prompt_template.format(query=query))
```
这段代码的主要目的是创建一个基于少量样本的提示模板，用于指导语言模型如何回应用户的查询。它首先定义了一些示例对话，然后创建了一个包含这些示例的模板。这个模板包括一个前缀、一个后缀和示例之间的分隔符。前缀用于给模型提供背景信息（即与AI助手的对话风格），后缀用于指示用户的输入和AI的响应位置。最后，代码使用了一个具体的查询来格式化这个模板，并打印出结果。这个结果可以直接用作语言模型的输入，以生成相应的响应。

### 7.2.3 部分格式化的提示模板详解
当我们在不同时间点获取到某些变量时，部分格式化的提示模板就显得尤其有用。它允许逐步构建提示，而不必等待所有必要的信息都集齐后再一次性处理。

以具体情境为例，假设有一个提示模板，它需要两个变量：foo和baz。在处理流程中，可能在较早的阶段就已经获取了foo的值，而baz的值则要稍后才能获得。在这种情况下，如果等到同时拥有这两个变量才将它们传递给提示模板，可能会使流程变得复杂和低效。

为了避免这种不便，可以使用部分格式化的提示模板。一旦获得了foo的值，就立即使用它来部分格式化提示模板。随后，当获得baz的值时，只需将部分格式化的提示模板与baz结合，即可完成整个提示的构建。下面演示一个执行此操作的示例：
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(template="{foo}{bar}", input_variables=["foo", "bar"])
partial_prompt = prompt.partial(foo="foo");
print(partial_prompt.format(bar="baz"))
```
或者也可以只使用部分变量初始化Prompt：
```python
prompt = PromptTemplate(template="{foo}{bar}", input_variables=["bar"], partial_variables={"foo": "foo"})
print(prompt.format(bar="baz"))
```
通过这种方式，部分格式化的提示模板提供了更大的灵活性和效率，能够在信息逐步变得可用时逐步构建提示，从而优化了处理流程。

部分格式化提示模板的另一个常见的应用场景是利用函数实现提示的部分化。这种情况通常出现在我们总是希望以某种固定方式获取某个变量时。

以日期或时间为例，这是一个非常贴切的场景。假设有一个提示，它总是需要包含当前的日期。直接在提示中硬编码日期显然是不可取的，因为这会导致提示很快过时。同时，将日期与其他输入变量一起传递，也可能会使事情变得复杂。

在这种情况下，使用一个函数来动态生成当前日期，并据此对提示进行部分格式化，就显得非常便捷。这样，每当需要该提示时，它都会自动包含最新的日期信息，无须手动更新或传递额外的参数。这不仅提高了提示的灵活性和实用性，还确保了信息的准确性和时效性。代码如下：
```python
from datetime import datetime
from langchain.prompts import PromptTemplate

def _get_datetime():
    now = datetime.now()
    return now.strftime("%m/%d/%Y, %H:%M:%S")

prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective", "date"]
);
partial_prompt = prompt.partial(date=_get_datetime)
print(partial_prompt.format(adjective="funny"))
```
更值得一提的是，在这种工作流中，读者也可以使用部分变量初始化Prompt，这通常更有意义。
```python
prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective"],
    partial_variables={"date": _get_datetime}
);
``` 
