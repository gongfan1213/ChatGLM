#### 4.1.4 从一个简单的提示模板开始
在自然语言生成任务中，生成高质量的文本是非常困难的，尤其是需要针对不同的主题、情境、问题或任务进行文本生成时，需要花费大量的时间和精力去设计、调试和优化模型，而且这种方式并不是高效的解决方案。因此，为了更好地服务ChatGLM3的处理，LangChain提供了一种名为PromptTemplates的提示文本技术，可以大大降低模型设计、调试和优化的成本。

PromptTemplates是一种可复制的、生成Prompt的方式，它包含一个文本字符串，可以接收来自终端用户的一组参数，并生成Prompt。PromptTemplates可以包含指令、少量示例和一个向语言模型提出的问题。我们可以使用PromptTemplates技术来指导语言模型生成更高质量的文本，从而更好地完成任务。

当用户和大语言模型对话时，用户所说的内容就是Prompt，即提示语。如果用户每次都需要输入很多内容相似的Prompt时，可以考虑生成一个Prompt模板，这样可以节省很多时间，而不必输入很多内容相似的Prompt。一个简单的示例如下：
```python
from langchain.prompts import PromptTemplate
template = """
请给我创建一个适配于{product}产品说明的模板
"""
prompt = PromptTemplate(
    input_variables=["product"],
    template=template,
)
prompt.format(product="colorful Tshirt")
```
这是一个最简单的输入Prompt，其作用是对输入的内容进行整理，从而根据设定进行计算和输出。同样，Prompt还可以适配更多的参数，从而构成一个更合格的模板，代码如下：
```python
template = """
请给我创建一个适配于{product}说明的模板，注意这个产品的颜色为{color}
"""
prompt = PromptTemplate(
    input_variables=["product","color"],
    template=template,
)
print(prompt.format(product="Tshirt", color="RED"))
```
上面代码创建了一个接收两个参数的提示模板，并通过参数的输入创建了一个完整的提示语句。结果如下：


请给我创建一个适配于Tshirt说明的模板，注意这个产品的颜色为RED

为了在更多场合下使用模版，还可以对预设模板的保存与载入方法，代码如下：
```python
from langchain.prompts import PromptTemplate
template = """
请给我创建一个适配于{product}说明的模板，注意这个产品的颜色为{color}
"""
prompt = PromptTemplate(
    input_variables=["product","color"],
    template=template,
)
print(prompt.format(product="Tshirt", color="RED"))
prompt.save("awesome_prompt.json") # Save to JSON file

from langchain.prompts.loading import load_prompt
prompt_load = load_prompt("awesome_prompt.json")
print(prompt_load.format(product="Tshirt", color="RED"))
```
结果如图4-3所示。两条打印语句分别打印出原始Prompt与载入的Prompt，此时原始的Prompt和通过载入方式获取的Prompt在输出形式上相同。

![image](https://github.com/user-attachments/assets/3cb5da62-c622-4df4-9b8d-bb1dd38dba37)


请给我创建一个适配于Tshirt说明的模板，注意这个产品的颜色为RED

请给我创建一个适配于Tshirt说明的模板，注意这个产品的颜色为RED


#### 4.1.5 ChatGLM3格式化提示词的构建与使用
我们的目标是让基于ChatGLM3设计的LLM终端，能够根据函数的名称生成对应的自然语言语义解释。为了实现这一目标，将设计一个专门的提示模板。这个模板的特别之处在于，它接收函数名称作为输入，并通过特定的格式展示函数的源代码。

在这样的场景中，自定义提示模板就显得尤为重要。通过自定义提示模板，可以确保模板内容更加贴近实际需求，从而使得LLM能够生成更加准确和更有用的解释。

具体实践上，为了创建自定义文字串模板，需要满足两个条件：
- 首先，该模板必须具备input_variables属性，以便明确提示模板所期望的输入变量。
- 其次，该模板需要公开format方法。这个方法接收与预期的input_variables相匹配的关键字参数，并返回经过格式化处理的提示。

接下来，我们将着手创建一个自定义提示模板，它以函数名称为输入，通过格式化处理来展示函数的源代码。

PromptTemplate的作用是生成对应的模板内容，从而在模型的调用上节省时间。一个简单的示例如下：
```python
from langchain import PromptTemplate
# 没有输入变量的示例Prompt
no_input_prompt = PromptTemplate(input_variables=[], template="给我讲个笑话。")
no_input_prompt.format()
```
这是来完成没有输入变量示例的PromptTemplate，当然读者也可以使用带有多个变量输入的Prompt来完成模板的创建。
```python
from 新第四章_Langchain知识图谱 import llm_chatglm

llm = llm_chatglm.ChatGLM()
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["location","street"],
    template="作为一名专业的旅游顾问，简单地说一下{location}有什么好玩的景点，特别是在{street}就可以。"
)
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
print(chain.run({"location":"南京","street":"新街口"}))
```
在上面代码中，我们设置了一个专业性质的Prompt Template，并将它传递给LLM终端进行回答，chain = LLMChain(llm=llm,prompt=prompt)是根据模型位置传递了对应的模型以及查询模版，而在具体使用时，需要将查询的内容按名称对应，以dict的格式插入模板中。请读者仔细查看打印结果。

LangChain的一大特点是：除了可以进行文本内容的查询外，还可以充分利用大模型的语言发现和逻辑能力，完成更进一步的内容。有时候，我们需要给LLM零学习样本，让LLM学习这些样本以便能够更加准确地回答问题，这被称为微调（fine-tune）LLM，为此可以使用Prompt的小样本模板来训练LLM。下面来看一个简单的示例，要求用户每输入一个短语，LLM就输出一个对应的反义词。首先定义两组输入输出的例子：
```python
from langchain.prompts import PromptTemplate,FewShotPromptTemplate
examples = [{"输入": "高兴", "输出": "悲伤"},{"输入": "高大", "输出": "低矮"}]
example_prompt = PromptTemplate(
    input_variables=["输入", "输出"],
    template="\n输入: {输入}\n输出: {输出}\n",
)
print(example_prompt)
```
然后是将它们组建成小样本学习的模板对象，代码如下：
```python
# 创建一个短语prompt模板对象
few_shot_prompt = FewShotPromptTemplate(
    # 这些是要插入prompt中的示例
    examples=examples,
    # 将示例插入prompt时，格式化示例的方式
    example_prompt=example_prompt,
    # 输入变量是用户直接输入的变量
    input_variables=["input"],
    # 前缀变量
    prefix="给出每个输入词语的反义词",
    # 后缀变量
    suffix="输入: {input}\n输出:",
    # 用来连接前缀、示例和后缀的字符串。
    example_separator="\n",
)
print(few_shot_prompt.format(input="快乐"))
```

![image](https://github.com/user-attachments/assets/a48d51f5-d9d3-49c9-a7e7-81b8fe56ce67)


打印结果如图4-4所示。
```
给出每个输入词语的反义词
输入: 高兴
输出: 悲伤
输入: 高大
输出: 低矮
输入: 快乐
输出: 
```
接下来，要在LangChain对接LLM时使用自定义的小样本提示语模板，这样LLM就可以根据小样本提示语模板的格式和要求来返回对应的内容：
```python
chain = LLMChain(llm=llm, prompt=few_shot_prompt)
print(chain.run("痛苦"))
print(chain.run("美丽"))
```
最终打印结果请读者自行查看。

通过使用LangChain，读者可以学习到如何标准化地完成提示语的输入和输出，从而生成符合项目需求的结果和答案。

### 4.2 ChatGLM3+LangChain搭建专业问答机器人
在前面的章节中，已经向读者展示了如何使用基础的ChatGLM3模型来完成一般性的知识问答任务。通过直接的Prompt方式，我们可以将所提问题有效地传递给ChatGLM3，并且观察到它在处理这类内容时展现出了相当不错的效果。

然而，当问题转向更为专业的领域，尤其是那些ChatGLM3未经数据训练的知识范畴时，其问答能力会面临怎样的挑战呢？这也正是本节将要深入探讨的问题。

为了更全面地评估ChatGLM3在专业领域的知识问答表现，我们将引入LangChain这一强大的工具。通过将ChatGLM3与LangChain相结合，期望能够提升模型在处理专业问题时的准确性和深度。LangChain的灵活性和可扩展性使其成为一个理想的选择，它能够帮助我们有效地整合和利用外部知识资源，从而弥补ChatGLM3在某些专业知识上的不足。

本节将详细展示如何结合ChatGLM3和LangChain来构建一个针对专业领域知识问答的解决方案。我们将分析这一组合在处理未经训练的专业知识时的表现，并讨论其潜在的优势和局限性。通过这一探索性的研究，期望为读者提供一个更全面、更深入的讲解，以便帮助读者在实际应用中更好地利用这些工具来解决专业领域的知识问答问题。

#### 4.2.1 使用LangChain的LLM终端完成文本问答
我们在前面制作了LangChain的LLM终端，可以通过它来完成终端的问答，代码如下：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()
print(llm("小孩牙龈肿痛服用什么药"))
```
注意，为了运行这个示例代码，需要启动4.1.2节的ChatGLM3内网服务代码。

![image](https://github.com/user-attachments/assets/a76935a6-0052-4209-a1bc-3d8bc95891da)


上面代码准备实现一个最普通的、涉及生活类的医学问答，即“小孩牙龈肿痛服用什么药”。在这里，我们使用已有的LLM终端完成此问题的回答，结果如图4-5所示。注意，在使用LLM终端进行问题回答时，每一次的结果会略有不同。

牙龈肿痛可能是由多种原因引起的，如牙结石、龋齿、牙菌斑等。在咨询医生之前，可以先用一些非处方药物缓解疼痛，如：

1. 对乙酰氨基酚（泰诺林）：这是一种非处方药，可以缓解牙痛和轻到中度的疼痛。可以每次服用500毫克，每4-6小时使用一次。但是，不应该给12岁以下的儿童使用，因为可能会引起肝损伤。

2. 非处方抗药药：如布洛芬（Advil、Motrin），可缓解牙痛和轻到中度的疼痛。可以每次服用200毫克，每6-8小时使用一次。但是，也应该避免给12岁以下的儿童使用，因为他们可能会有胃肠道反应。

3. 局部麻醉剂：如局部麻醉膏（Orajel），可以在牙龈上涂抹麻醉膏，应该按照使用说明使用。



然而，这些药物只是缓解疼痛的临时方法。如果正在经历牙龈肿痛，建议尽快咨询医生。

这是一个较经典的回答，其中涉及用药建议，但是并没有直接深入回答我们所提出的问题，即“服用什么药”。相对于使用大模型训练的结果，我们同时也准备了一份专业回答，如图4-6所示。

context_text:"牙龈肿痛会给我们日常生活带来很大的苦恼，为在这里汇聚了相关知识。对于牙龈肿痛一定要注意做好相关工作，牙龈肿痛的发病源非常广泛，孩子在以后也会遇到此类痛苦。如果孩子不幸患上牙龈肿痛，需要尽快去接受正规治疗，并且维护好口腔卫生。牙龈肿痛又称为了牙痛，也就是你牙龈疼痛并且伴随着它周围的齿龈肿痛。小孩发生牙龈肿痛的原因比较多，不同的情况对应着不同的治疗方法。当孩子出现这种症状后，需要尽快去检查是因为哪种原因，处理小孩牙龈肿痛的三个方法：1. 牙齿上有牙垢、牙石需要及时清除。牙齿上有牙垢、牙石会引起牙龈肿痛发炎。这时可以采用盐水漱口，用盐水漱口是因为盐水有杀菌的作用。还可以采用大蒜止痛，把大蒜捣烂温热后敷在痛点上，这是最常用的方法。孩子若是发生牙龈肿痛，要及时给孩子使用盐水漱口。家里如果有蒜，就把蒜捣烂温热后敷在痛点上，这是最常用的方法。大蒜对牙齿有比较严重的伤害，并且有明确龋齿的孩子，家长们就可以用大蒜反复去摩擦牙龈疼痛区，每天摩擦两次，等一周之后酸痛感会明显减轻。牙龈肿痛也说不上是什么特别严重的疾病，但是它的危害也比较大，为此在这里提醒广大家长朋友们，如果孩子一旦出现这种情况，就应该带孩子去医院接受检查，因为牙龈肿痛发生的原因比较多，只有检查出什么原因引起的，才能够对症治疗。"

标记部分的文字明确指出了针对小孩牙龈肿痛问题的一种传统治疗方案——服用牛黄解毒丸。我们的期望是，大语言模型终端能够基于我们提供的文本资料，准确、高效地回答相应的问题，且答案直接来源于文本内容。

接下来，我们探讨使用LLM终端根据文本回答问题的策略。最直观的方法是将整个文档输入LLM终端，并通过精确的提示引导它在文档中查找并回答特定问题。然而，这种方法在实际操作中并不切实可行。

其主要问题在于，这种方法需要处理的文档量可能非常庞大，从而严重消耗硬件资源，尤其是显存。此外，大量的数据会显著增加LLM终端的处理时间，导致响应速度变慢。更严重的是，过多的信息可能会干扰LLM终端对查询范围的准确判断，进而影响答案的质量。

鉴于上述局限性，我们需要寻找一种新的解决方案。前文已经分析了此方案不可行的主要原因在于需要读取的文档长度过长，这对LLM终端的硬件和软件条件都提出了挑战。那么，我们是否可以考虑只将与问题最相关的“部分文档”信息发送给LLM终端呢？

这种思路的可行性在于，通过减少输入信息的量，可以有效降低硬件资源的消耗，同时提高LLM终端的处理速度。更重要的是，更聚焦的输入信息有助于LLM终端更准确地判断查询范围，从而给出更高质量的答案。在接下来的探讨中，我们将进一步验证这种策略的有效性，并探索实施过程中的具体细节和可能面临的问题。 
