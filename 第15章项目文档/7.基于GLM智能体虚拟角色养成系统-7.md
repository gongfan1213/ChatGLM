### 图15 - 42 将GitHub仓库的代码下拉至本地
```
(base) shuzhimin@218keg:~$ git clone https://github.com/shuzhimin/CharacterAI.git
Cloning into 'CharacterAI'...
remote: Enumerating objects: 2298, done.
remote: Counting objects: 100% (486/486), done.
remote: Compressing objects: 100% (292/292), done.
remote: Total 2298 (delta 187), reused 352 (delta 182), pack - reused 1812
Receiving objects: 100% (2298/2298), 63.78 MiB | 6.92 MiB/s, done.
Resolving deltas: 100% (1352/1352), done.
```


![image](https://github.com/user-attachments/assets/8bb1c95f-4950-45bf-8376-202a97f87347)

### 15.7.2 部署
#### 1. 安装Qdrant、PostgreSQL和Minio

在部署案例系统之前，读者需要先在服务器上用docker安装Qdrant、PostgreSQL和Minio。

Qdrant是一个向量数据库，PostgreSQL是一个关系型数据库，Minio是一个开源的对象存储服务器。进入下载好的CharacterAI文件夹内，可以看到有一个docker - compose.yaml文件，这是用于部署qdrant、dpostgresql和minio的配置文件，读者可以在里面指定qdrant、dpostgresql和minio的端口号、用户名和密码。当前文件中的postgres数据库的用户名为ysukeg，密码为123456，数据库名称为characterai，端口为5432；minio的用户名为root，密码为12345678，端口为9000，如图15 - 43所示。
```
(base) shuzhimin@218keg:~$ cd CharacterAI/
(base)_shuzhimin@218keg:~/characterAI$ ls
app docker - compose.yaml example - conf.tool front langchain_gpt3 tuning README.md requirements.txt test
(base)_shuzhimin@218keg:~/CharacterAI$ cat docker - compose.yaml
services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: qdrant
    ports:
      - 6333:6333
      - 6334:6334
  postgres:
    image: postgres
    container_name: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      # all set to default so you could just type
      # postgres to connect
      POSTGRES_USER:ysukeg
      POSTGRES_PASSWORD:123456
      POSTGRES_DB:characterai
  minio:
    image: bitnami/minio
    container_name: minio
    restart: always
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER:root
      # Make sure that the environment variables MINIO_ROOT_PASSWORD and MINIO_SERVER_SECRET_KEY meet the strength requirements enforced by Minio(P).
      MINIO_ROOT_PASSWORD:12345678
```

![image](https://github.com/user-attachments/assets/09b6e166-e124-4ac8-9538-e1220441b47c)

### 图15 - 43 docker - compose.yaml文件

在服务器的命令窗口中输入“docker compose -f docker - compose.yaml up -d”就能够自动安装并启动Qdrant、PostgreSQL和Minio。其中 -f docker - compose.yaml表示docker compose配置文件的路径为docker - compose.yaml，up表示启动docker应用程序的服务，-d表示在后台运行服务。读者可以执行“docker ps”查看Qdrant、PostgreSQL和Minio是否成功启动，图15 - 44表示已经成功启动。

```
(base) shuzhimin@218keg:~/CharacterAI$ docker compose -f docker - compose.yaml up -d
[+] Running 3/3
 ⠿ Container qdrant  Started
 ⠿ Container minio   Started
 ⠿ Container postgres Started
(base) shuzhimin@218keg:~/CharacterAI$ docker ps
CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS
00e27f72a618b   qdrant/qdrant:latest   "./entrypoint.sh"        25 seconds ago   Up 24 seconds   0.0.0.0:6333 - 6333/tcp, 0.0.0.0:6334 - 6334/tcp
6e5b3a649c97    postgres            "docker - entrypoint.s…"   25 seconds ago   Up 24 seconds   0.0.0.0:5432 - 5432/tcp
777b24a7d9e8    bitnami/minio       "/opt/bitnami/script.l…"   25 seconds ago   Up 24 seconds   0.0.0.0:9000 - 9000/tcp, 0.0.0.0:9001 - 9001/tcp
```


![image](https://github.com/user-attachments/assets/7232fc6e-012e-456a-87b4-ec72aa982838)


### 图15 - 44 使用docker compose执行配置文件

#### 2. 部署微调后的ChatGLM2 - 6B

在安装完Qdrant、PostgreSQL和Minio之后，读者需要部署微调后的ChatGLM2 - 6B，ChatGLM2 - 6B的微调过程已经在第14章ChatGLM微调中给出，下面将直接介绍在该应用案例中该如何部署微调后端模型。

进入CharacterAI/ptuning路径下，可以看到有character_llm_api.py、README.md、requirements.txt和tuned_model文件夹（见图15 - 45）。character_llm_api.py文件是用于将微调好的模型变成一个可供调用的FastAPI接口；README.md内给出了部署微调后模型的步骤；requirements.txt内是部署的Python环境中需要的包；tuned_model文件夹用于存放微调好的模型文件。
```
(base) shuzhimin@218keg:~/CharacterAI$ cd ptuning/
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ ls
character_llm_api.py README.md requirements.txt tuned_model
```

![image](https://github.com/user-attachments/assets/93f5ba78-3391-4cad-bbc9-914f91523490)

### 图15 - 45 ptuning文件夹内的文件


![image](https://github.com/user-attachments/assets/5a437dd5-073e-4aec-979a-6cc7690ca6ad)


首先，读者需要下载ChatGLM2 - 6B的模型文件。进入CharacterAI/ptuning路径下，在命令窗口中执行“git lfs install”和“git clone https://huggingface.co/THUDM/chatglm2 - 6b”即可完成下载，如图15 - 46所示。由于模型较大，下载过程可能需要一段时间，请耐心等待。如果下载很慢，也可以选择先将ChatGLM2 - 6B模型文件手动下载到本地，存放在以模型名字命名的文件夹内（下载地址：https://huggingface.co/THUDM/chatglm2 - 6b/tree/main），如图15 - 47所示。然后通过scp命令上传至服务器中相应路径下（要上传至项目文件中的ptuning文件夹下），如图15 - 48所示，其中“-r”表示递归复制，上传文件夹时使用；“./chatglm2 - 6b”是本地源文件夹的路径；“shuzhimin@211.81.248.218”是远程服务器的用户名和IP地址，告诉scp连接到哪个服务器以及使用哪个用户账户；“/home/shuzhimin/test/CharacterAI/ptuning”是远程目标路径。
```
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ git lfs install
Updated git hooks.
git LFS initialized.
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ git clone https://huggingface.co/THUDM/chatglm2 - 6b
Cloning into 'chatglm2 - 6b'...
remote: Enumerating objects: 186, done.
remote: Counting objects: 100% (126/126), done.
remote: Compressing objects: 100% (88/88), done.
remote: Total 186 (delta 88), reused 38 (delta 38), pack - reused 69 (from 1)
Receiving objects: 100% (186/186), 1.96 MiB | 332.09 KiB/s, done.
Resolving deltas: 100% (88/88), done.
```

### 图15 - 46 ChatGLM2 - 6B下载

### 图15 - 47 手动下载ChatGLM2 - 6B到本地
```
(base) shuzhimin@Desktop/model$ scp -r./chatglm2 - 6b shuzhimin@211.81.248.218:/home/shuzhimin/test/CharacterAI/ptuning
shuzhimin@211.81.248.218's password:
config.json                                                           100%  3118     82.3KB/s   00:00
configuration_chatglm.py                                              100%  2332     291.7KB/s   00:00
modeling_chatglm.py                                                   100%  14359    1.7MB/s   00:00
pytorch_model - 00001 - of - 00007.bin                                   100%  127395   7.7MB/s   00:40
pytorch_model - 00002 - of - 00007.bin                                   100%  187975   7.2MB/s   00:49
pytorch_model - 00003 - of - 00007.bin                                   100%  131274   6.3MB/s   00:37
pytorch_model - 00004 - of - 00007.bin                                   100%  131274   6.3MB/s   00:37
pytorch_model - 00005 - of - 00007.bin                                   100%  131274   6.3MB/s   00:37
pytorch_model - 00006 - of - 00007.bin                                   100%  131274   6.3MB/s   00:37
pytorch_model - 00007 - of - 00007.bin                                   100%  131274   6.3MB/s   00:37
pytorch_model.bin                                                     100%  547MB  7.9MB/s   02:31 ETA
```
### 图15 - 48 使用scp将本地文件上传至服务器

![image](https://github.com/user-attachments/assets/c02a1752-8307-40bb-99bb-c055c201cf05)


下载完成后，将能够看到在ptuning文件夹内多出来一个ChatGLM2 - 6B的文件夹，如图15 - 49所示。然后，读者需要配置Python环境。在命令行窗口中执行“conda create -n glm2 python=3.10”，如图15 - 50所示。其中glm2是环境名称，python = 3.10是选择的Python版本为3.10。创建好conda环境后，执行“conda activate glm2”进入环境中，如图15 - 51所示。
```
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ ls
character_llm_api.py chatglm2 - 6b README.md requirements.txt tuned_model
```
### 图15 - 49 查看下载的ChatGLM2 - 6B
```
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ conda create -n glm2 python=3.10
Channels:
  - defaults
Platform: linux - 64
Collecting package metadata (repodata.json): done
Solving environment: done
```

![image](https://github.com/user-attachments/assets/ce6a02c8-aedf-4660-81d1-824706015bea)


### 图15 - 50 创建conda环境
```
(base) shuzhimin@218keg:~/CharacterAI/ptuning$ conda activate glm2
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ []
```
### 图15 - 51 进入conda环境

进入环境后，读者需要安装PyTorch，在命令窗口中执行“conda install pytorch torchvision torchaudio pytorch - cuda=12.1 -c pytorch -c nvidia”进行安装，如图15 - 52所示。安装完PyTorch之后，还需要安装其他需要的Python包。在ptuning路径下，执行“pip install -r requirements.txt”进行安装，如图15 - 53所示。
```
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ conda install pytorch torchvision torchaudio pytorch - cuda=12.1 -c pytorch -c nvidia
Channels:
  - pytorch
  - nvidia
  - defaults
Platform: linux - 64
Collecting package metadata (repodata.json): done
Solving environment: done
```
### 图15 - 52 安装PyTorch
```
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ pip install -r requirements.txt
Collecting transformers==4.30.2 (from -r requirements.txt (line 1))
  Using cached transformers - 4.30.2 - py3 - none - any.whl.metadata (113 kB)
Requirement already satisfied: torch>=2.0.0 in /home/shuzhimin/anaconda3/envs/glm2/lib/python3.10/site - packages (from -r requirements.txt (line 2))
Collecting fastapi==0.111.0 (from -r requirements.txt (line 3))
  Using cached fastapi - 0.111.0 - py3 - none - any.whl.metadata (25 kB)
Collecting uvicorn[standard]==0.23.2 (from -r requirements.txt (line 4))
  Using cached uvicorn - 0.23.2 - py3 - none - any.whl.metadata (6.3 kB)
Collecting pydantic (from -r requirements.txt (line 5))
  Using cached pydantic - 2.7.1 - py3 - none - any.whl.metadata (197 kB)
Collecting sentencepiece (from -r requirements.txt (line 6))
  Using cached sentencepiece - 0.2.0 - cp310 - cp310 - manylinux2 _ x86_64.manylinux2014_x86_64.whl.metadata (218 kB)
Collecting accelerate (from -r requirements.txt (line 7))
  Using cached accelerate - 0.30.1 - py3 - none - any.whl.metadata (18 kB)
```

![image](https://github.com/user-attachments/assets/ab64d752-212b-494d-a3c7-fa94dda41e83)


### 图15 - 53 安装需要的Python包

最后，为了防止服务器中代理的影响，需要执行“export no_proxy="211.81.248.218"”，之后再执行“nohup python character_llm_api.py > character_llm_api.log 2>&1 &”将程序挂在后台即可完成微调后的ChatGLM2 - 6B的部署，然后执行“cat character_llm_api.log”，查看是否部署成功，与图15 - 54中一样表示成功部署在了8086端口。读者可以在浏览器中输入“211.81.248.218:8086/docs”查看FastAPI提供的接口文档，“211.81.248.218”是该服务器的IP，读者需要换成自己所使用服务器的IP。
```
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ export no_proxy="211.81.248.218"
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ nohup python character_llm_api.py > character_llm_api.log 2>&1 &
[1] 216876
(glm2) shuzhimin@218keg:~/CharacterAI/ptuning$ cat character_llm_api.log
nohup: ignoring input
[1] 216876 7 /7 [06:06:00, 0.021z/s]
Loading checkpoint shards: 100%|███████████████████████████████████████| 0/0 [00:00<?, ?it/s]
Some weights of ChatGLM2 - 6B were not initialized from the model checkpoint at./chatglm2 - 6b
and are newly initialized: ['transformer.encoder.embedding.weight']
You should probably TRAIN this model on a down - stream task to be
```

![image](https://github.com/user-attachments/assets/39a157ff-2978-45e6-b059-952709f1afe8)
