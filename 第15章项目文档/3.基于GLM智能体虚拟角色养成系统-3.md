### 基于GLM智能体虚拟角色养成系统

1. 接口5.4 查询智能体接口
    - **功能描述**：用户管理员查询智能体，支持模糊查询。
    - **参数**：见表15 - 20。

### 表15 - 20 查询智能体接口的参数

| 参数名 | 参数简介 | 是否必选 | 类型 |
| ---- | ---- | ---- | ---- |
| query | 查询的智能体名 | 否 | str |
| page_num | 页码 | 否 | int |
| page_size | 每页数量 | 否 | int |
    - **返回值**：
```json
{
    "characters": [
        {
            "cid": 0,
            "name": "string",
            "description": "string",
            "category": "string",
            "avatar_description": "string",
            "avatar_url": "string",
            "created_at": "2024-05-12T15:01:54.229Z",
            "updated_at": "2024-05-12T15:01:54.229Z",
            "is_shared": true,
            "uid": 0
        }
    ],
    "scores": [
        0
    ],
    "total": 0
}
```
2. **接口实现**
    - **（1）后端开发的文件目录结构**

![image](https://github.com/user-attachments/assets/d2df1d6b-2835-4b71-a102-d844290995ed)


首先，在工作路径下创建项目文件夹（例如app文件夹），进入app文件夹内后，再分别创建`__init__.py`文件、routers文件夹、common文件夹、database文件夹和llm文件夹，并且分别在routers、common、database和llm文件夹内创建`__init__.py`文件，最终整个项目的文件目录结构如图15 - 15所示。
    - **下面将解释每个文件和文件夹的作用**：
        
        1. `app/`是整个应用程序的根目录。
        
        2. `common/`是用来存放应用程序中共享的通用函数，如配置文件`conf.py`等。
        
        3. `database/`通常用于存放数据库相关的代码。
        
        4. `llm/`用来存放大模型相关代码。
        
        5. `router/`通常用于存放应用程序的路由模块。
        
        6. `dependencies.py`用于定义应用程序的依赖项，例如数据库连接、身份验证等。
        
        7. `__init__.py`通常用于将目录变为Python包，以便可以被其他文件导入。
        
        8. `main.py`是应用程序的主文件，通常包含应用程序的入口点，用于初始化FastAPI应用程序，并定义路由、中间件等。
        
        上面的文件目录结构是开始后端开发的时候首先要创建的，之后只需要在对应的文件夹下补充相应的功能即可，图15 - 16是后端开发完成后的完整目录结构。
        
        其中aibot文件夹内存放的是如何实现角色扮演智能体（`character.py`）、报表智能体（`reporter.py`）和长文档解读智能体（`retriever`）的核心代码。llm文件夹存放了实现智能体功能的部件。
    - **（2）角色扮演智能体实现**

角色扮演智能体的核心是使用角色数据集微调过的ChatGLM2 - 6B模型，在第14章ChatGLM微调中已详细介绍过是怎么进行微调的。由于经过角色数据的微调，使得ChatGLM2 - 6B能够根据用户设定的角色名字和角色描述进行符合角色人设的对话。本书将微调好的ChatGLM2 - 6B封装成了一个接口，`character_llm()`就是用于调用接口的函数，也就是角色扮演智能体。用户发送的消息传入`ainvoke()`中，然后在内部调用角色扮演智能体，最后将智能体的回复再传回给用户。具体实现如下：

![image](https://github.com/user-attachments/assets/17d4d97d-e989-4739-9ea6-de4d54d26382)


```python
from typing import AsyncGenerator
import requests
from app.common.model import (
    ChatMessage,
    RequestItemMeta,
    RequestItemPrompt,
    RequestPayload,
    ResponseModel
)
from app.database.schema import Message
from app.llm.glm import character_llm
from.interface import AIBot

class RolePlayer(AIBot):
    def __init__(self, cid: int, meta: RequestItemMeta, chat_history: list[Message]):
        self.meta = meta
        self.chat_history: list[RequestItemPrompt] = []
        for message in chat_history:
            role = message.sender
            content = message.content
            self.chat_history.append(RequestItemPrompt(role=role, content=content))

    async def ainvoke(self, input: ChatMessage) -> AsyncGenerator[ChatMessage, None]:
        self.chat_history.append(RequestItemPrompt(role="user", content=input.content))
        response_content = character_llm(
            payload=RequestPayload(meta=self.meta, prompt=self.chat_history)
        ).message
        self.chat_history.append(
            RequestItemPrompt(
                role="assistant",
                content=response_content
            )
        )
        yield ChatMessage(
            sender=input.receiver,
            receiver=input.sender,
            is_end_of_stream=True,
            content=response_content
        )
```
    - **（3）报表智能体实现**
报表智能体的实现要借助于LangChain。在第13章ChatGLM部署中介绍到，LangChain是一个用于开发由语言模型驱动的应用程序的框架，能够很方便地帮人们开发大模型的应用程序。报表智能体的实现分为四个阶段。
        - **第一阶段为“检验输入是否符合功能设定”**。首先设定用于检验输入的提示（下方代码中的`check_template`），然后通过LangChain提供的语言链接交换层（Language Chain Exchange Layer, LCEL）将提示与ChatGLM3 - 6B模型组成一个链（下方代码中的chain），最后用`invoke()`来接收用户的内容并激活chain到ChatGLM3 - 6B的响应。根据提示的设计，模型只会回复“yes”与“no”。如果是“yes”，则输入与报表功能相符，继续进行后续阶段。否则，给用户返回“非常抱歉，我只能回答编写代码绘制图表的相关问题”。这样，就能限定模型仅回复与报表相符的话题，使其符合报表智能体的特性。
        - **第二阶段为“获取相应的数据”**。报表智能体绘制报表需要结合实际的用户数据，所以报表智能体应该能从用户的问题中自动识别出需要数据库中的哪些数据。在获取数据之前，需要让报表智能体思考该用户是否有数据，如果有数据才需要获取，这样使得智能体更具智能。首先，设定好用于从数据库中获取数据的提示（`get_data_prompt`）。然后，用到了LangChain提供的利用大模型接入数据库的库函数`QuerySQLDatabaseTool`和`SQLDatabase`，并且根据LangChain的语法将提示、数据库和大模型链接起来得到chain，最后通过`invoke()`接收输入来得到模型的回复。这里总共问了两次模型，第一次问模型该用户有没有数据，如果没有则回复“no”，如果有则进行第二次问答，让模型根据用户输入从数据库中获取绘制相应报表会用到的数据。
        - **第三阶段为“获取数据的文本描述”**。获取到的数据需要让模型自动整理成一段话返回给用户。也是分为三段式：先设定提示，然后与模型链接，最后根据问题得到模型响应。
        - **第四阶段为“根据获取到的数据编写绘图代码并执行”**。第四阶段接收第二阶段获得的数据，并结合用户的问题，利用模型生成代码，最后利用LangChain提供的PythonREPL库函数来执行模型生成的代码，得到相应的报表。
        报表智能体的实现采用了分阶段式引导，ChatGLM3 - 6B给出我们想要得到的回答，赋予报表智能体决策能力。在代码实现中，本书在`reporter_llm()`函数中将四个阶段连接在一起，使其成为一个报表智能体，并将用户消息传递给`ainvoke()`函数，然后再调用报表智能体返回消息给用户。具体代码实现如下：
```python
import os
import uuid
from operator import itemgetter
from typing import AsyncGenerator
import requests
from langchain.chains import create_sql_query_chain
from langchain_community.chat_models.zhipuai import ChatZhipuAI
from langchain_community.llms.chatglm3 import ChatGLM3
from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool
from langchain_community.utilities import SQLDatabase
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (ChatPromptTemplate, MessagesPlaceholder,
                                    PromptTemplate)
from langchain_core.runnables import RunnablePassthrough
from langchain_experimental.utilities import PythonREPL

from app.common import conf
from app.common.minio import minio_service
from app.common.model import ChatMessage, ReportResponseV2
from.interface import AIBot


class Reporter(AIBot):
    def __init__(self, uid: int):
        self.uid = uid
        self.llm = ChatGLM3(
            temperature=0,
            endpoint_url="http://211.61.248.218:8000/v1/chat/completions"
        )
        self.db_url = conf.get_postgres_sqlalchemy_database_url()

    def check_question(self, question: str) -> str:
        """第一部分 检验输入是否符合功能设定"""
        check_template = """
        你只能回复“yes”和“no”。
        判断用户的输入是否与“生成代码绘制图表”有关。
        如果有关，你只需回答“yes”。
        如果无关，你只需回答“no”。
        """
        check_prompt = ChatPromptTemplate.from_messages(
            [("system", check_template), ("human", "{input}")]
        )
        chain = check_prompt | self.llm
        check_result = chain.invoke({"input": question})
        return check_result

    def get_data(self, question: str, uid: int) -> str:
        """第二部分 获取相应的数据"""
        db = SQLDatabase.from_uri(self.db_url)
        get_data_prompt = PromptTemplate.from_template(
            """给定用户问题、相应的SQL查询和SQL结果，回答用户问题。
            Question: {question}
            SQL Query: {query}
            SQL Result: {result}
            Answer: """
        )
        execute_query = QuerySQLDatabaseTool(db=db)
        write_query = create_sql_query_chain(self.llm, db)
        answer = get_data_prompt | self.llm | StrOutputParser()
        chain = (
            RunnablePassthrough.assign(query=write_query).assign(
                result=itemgetter("query") | execute_query
            )
            | answer
        )
        data_content = chain.invoke(
            {
                "question": f"""请判断characters表中是否有uid={uid}。
                如果没有，你只需要回复“no”。
                如果有，你只需要回复“yes”。
                请不要回复其他无关内容。"""
            }
        )
        data_question = f"""分析“{question}”应该需要用到哪些数据来绘图，并从characters表且uid={uid}中获取这些数据。你只需要用dict类型返回获取到的数据，不要输出其他无关信息。"""
        if data_content == "yes":
            data_content = chain.invoke({"question": data_question})
        return data_question, data_content

    def get_data_description(self, data_question: str, data_content: str) -> str:
        """第三部分 获取数据的文本描述"""
        # 参考资料: https://python.langchain.com/v0.1/docs/use_cases/chatbots/memory_management/
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a helpful assistant. Answer all questions to the best of your ability.",
                ),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )
        chain = prompt | self.llm
        data_description = chain.invoke(
            {
                "messages": [
                    HumanMessage(content=data_question),
                    AIMessage(content=data_content),
                    HumanMessage(content="用一句话提取和总结获取到的数据"),
                ],
            }
        )
        return data_description

    def _sanitize_output(self, text: str):
        # 从大模型的输出中提取出代码
        # _after =
        texts = text.split("```python")
        if len(texts) > 1:
            after = texts[-1]
        else:
            after = ""
        return after.split("```")[0]

    def code_plot(self, data_content: str, question: str, save_path: str) -> str:
        """第四部分 根据获取到的数据编写绘图代码并执行"""
        draw_template = f"""Write some Python code to help users draw graph.
        The graph drawn in Python must specify a save path.
        The path to save the graph is "{save_path}".
        Return only python code in Markdown format, e.g.:
        ```python
       ...
        ```"""
        prompt = ChatPromptTemplate.from_messages(
            [("system", draw_template), ("human", "{input}")]
        )
        chain = prompt | self.llm | StrOutputParser()
        output = chain.invoke(
            {
                "input": f"数据为{data_content}，请将数据改为字典类型，再根据数据编写代码完成“{question}”任务。一定要根据指定路径保存图片"
            }
        )
        code_text = self._sanitize_output(output)
        PythonREPL().run(code_text)
        return code_text

    def _generate_image_path(self) -> str:
        path_prefix = conf.get_save_image_path()
        os.makedirs(path_prefix, exist_ok=True)
        return os.path.join(path_prefix, f"{uuid.uuid4()}.png")

    def reporter_llm(self, question: str, uid: int) -> ReportResponseV2:
        # 根据用户的content问题和uid，生成对应报表
        flag = self.check_question(question)
        content = "非常抱歉，我只能回答编写代码绘制图表的相关问题"
        if flag == "yes":
            data_question, data_content = self.get_data(question, uid)
            # 初始化当数据没有获取到时content的值
            content = "很抱歉，没有查询到该用户有创建角色"
            # 判断用户是否有创建角色
            if data_content != "no":
                data_description = self.get_data_description(
                    data_question, data_content
                )
                save_path = self._generate_image_path()
                code_text = self.code_plot(data_content, question, save_path=save_path)
                content = data_description + "\n" + code_text
                if os.path.exists(save_path):
                    url = minio_service.upload_file(file=filename=save_path)
        return ReportResponseV2(content=content, url=url)

    async def ainvoke(self, input: ChatMessage) -> AsyncGenerator[ChatMessage, None]:
        response_content = self.reporter_llm(question=input.content, uid=self.uid)
        yield ChatMessage(
            chat_id=input.chat_id,
            sender=input.receiver,
            receiver=input.sender,
            is_end_of_stream=False,
            content=response_content.content,
            images=[response_content.url]
        )
```
    - **（4）长文档解读智能体实现**

长文档解读智能体也是根据LangChain实现，主要分为两部分。第一部分为“将问题置于上下文中”由下方代码中的`create_retriever()`实现，利用了LangChain提供的`create_history_aware_retriever()`库函数结合提示“`history_aware_prompt`”从知识库“`KnowledgeBase.as_retriever(knowledge_id=self.knowledge_id)`”中提取符合用户问题的上下文并返回该上下文和用户问题给第二部分。第二部分为“问答”由`create_qabot()`实现，接收来自第一部分的上下文和用户问题，将上下文与用户问题结合起来输入给模型，然后得到模型的回复。`create_rag()`通过`RunnableWithMessageHistory()`将第一部分和第二部分连接起来得到一个完整的长文档解读智能体。用户的消息传入`ainvoke()`，然后调用长文档解读智能体，再根据用户上传的知识库文档回答用户问题。

```python
from typing import AsyncGenerator
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import retrieval_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.chat_models.zhipuai import ChatZhipuAI
from langchain_community.chat_models import BaseChatMessageHistory
from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage, SystemMessage)
from langchain_core.prompts import ChatPromptTemplate, MessagesHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from app.common import conf, model
from app.common.model import ChatMessage
from app.database import schema
from app.llm import ZhipuAIEmbeddings
from.app import AIBot
from app.common.vector_store import KnowledgeBase


def from_schema_message_to_langchain_message(message: schema.Message) -> BaseMessage:
    match model.MessageSender(message.sender):
        case model.MessageSender.HUMAN:
            return HumanMessage(content=message.content)
        case model.MessageSender.AI:
            return AIMessage(content=message.content)
        case model.MessageSender.SYSTEM:
            return SystemMessage(content=message.content)
        raise ValueError(f"unsupported message type: {message}")


class RAG(AIBot):
    def __init__(
        self,
        cid: int,
        uid: int,
        chat_id: int,
        knowledge_id: str,
        chat_history: list[schema.Message] = [],
    ) -> None:
        assert knowledge_id, "empty knowledge id"
        self.cid = cid
        self.uid = uid
        self.knowledge_id = knowledge_id
        self.session_id = str(chat_id)
        self.model = ChatZhipuAI(
            temperature=0.95,
            model="glm-4",
            api_key=conf.get_zhipuai_key(),
        )
        self.embeddings = ZhipuAIEmbeddings(api_key=conf.get_zhipuai_key())
        self.store: dict[str, BaseChatMessageHistory] = {}
        history = self.get_session_history(session_id=self.session_id)
        for message in chat_history:
            history.add_message(
                message=from_schema_message_to_langchain_message(message=message)
            )
        self.rag = self.create_rag()

    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def create_retriever(self):
        history_aware_prompt = ChatPromptTemplate.from_messages(
            messages=[
                (
                    "system",
                    """Given a chat history and the latest user question \
                    which might reference context in the chat history, formulate a standalone question \
                    which can be understood without the chat history. Do NOT answer the question, \
                    just reformulate it if needed and otherwise return it as is.""",
                ),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        return create_history_aware_retriever(
            prompt=history_aware_prompt,
            llm=self.model,
            retriever=KnowledgeBase.as_retriever(knowledge_id=self.knowledge_id),
        )

    def create_qabot(self):
        # 2. QA chain
        return create_stuff_documents_chain(
            prompt=ChatPromptTemplate.from_messages(
                messages=[
                    (
                        "system",
                        """You are an assistant for question-answering tasks. \
                        Use the following pieces of retrieved context to answer the question. \
                        If you don't know the answer, just say that you don't know. \
                        Use three sentences maximum and keep the answer concise.\
                        {context}""",
                    ),
                    MessagesPlaceholder("chat_history"),
                    ("human", "{input}"),
                ]
            ),
            llm=self.model,
        )

    def create_rag(self):
        def get_session_history(session_id: str) -> BaseChatMessageHistory:
            if session_id not in self.store:
                self.store[session_id] = ChatMessageHistory()
            return self.store[session_id]

        return RunnableWithMessageHistory(
            runnable=create_retrieval_chain(
                retriever=self.create_retriever(),
                combine_docs_chain=self.create_qabot(),
            ),
            get_session_history=get_session_history,
            input_messages_key="input",
            output_messages_key="answer",
            history_messages_key="chat_history",
        )

    async def ainvoke(self, input: ChatMessage) -> AsyncGenerator[ChatMessage, None]:
        async for output in self.rag.ainvoke(
            {
                "input": input.content,
                # session id 应该是chatid 吧
                # 这是异步的但是不是多线程的 所以不用加锁
                "config": {"configurable": {"session_id": self.session_id}},
            }
        ):
            yield ChatMessage(
                sender=self.cid,
                receiver=self.uid,
                is_end_of_stream=False,
                content=output["answer"],
            )
        yield ChatMessage(sender=self.cid, receiver=self.uid, is_end_of_stream=True, content="")
```
上述仅给出了角色扮演智能体、报表智能体和长文档解读智能体的代码实现，还有在图15 - 16出现的其他功能的代码将在GitHub仓库中给出。

### 15.4.2 前端页面实现
#### （1）创建一个Vue项目工程
在第2章的时候已经介绍了如何配置Vue的环境，接下来本节只介绍如何为项目创建一个Vue项目工程。

首先，打开命令行窗口，输入“vue ui”，如图15 - 17所示。然后将自动打开浏览器进入Vue的可视化配置项目工程的界面，然后单击左下角的“Vue项目管理器”创建新的工程，如图15 - 18所示。

![image](https://github.com/user-attachments/assets/bd242b50-8a31-40a4-8285-6bd7d94aa495)


- 图15 - 17 进入Vue的UI页面
```
PS D:\shuzhimin\Desktop> vue ui
  Starting GUI...
  Ready on http://localhost:8000
```
- 图15 - 18 Vue的UI页面
（页面展示“欢迎来到新项目！”等相关信息 ，此处因图片未完整展示相关文字 ，按原文示意 ） 
