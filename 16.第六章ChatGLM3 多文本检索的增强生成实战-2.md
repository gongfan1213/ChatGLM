120 | ChatGLM3 大模型本地化部
# 2. 分词器（Text Splitting）
分词是一种精细处理长文本的技术，它能将文本拆解成更小的分析单元，如字、词和句。在LangChain中，TextSplitter 扮演了分词器的基石的角色，它是所有分词功能的起点。TextSplitter不仅灵活，还为用户提供了两个关键参数来调整分词的行为和结果，从而确保分词操作既精准又符合特定需求。这两个参数如下：
- chunk_size：文本分割的滑窗长度。
- chunk_overlap：重叠滑窗长度。
示例代码如下：
```python
class TextSplitter(BaseDocumentTransformer, ABC):
    """Interface for splitting text into chunks."""
    def __init__(
        self,
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: Callable[[str], int] = len,
    )
```
在具体在应用上，我们主要使用 CharacterTextSplitter 和 RecursiveCharacterTextSplitter。
首先来看 CharacterTextSplitter，它调用 Python 自带的 split()方法，可以用自定义分隔符进行文本切分，其部分源码如下：
```python
class CharacterTextSplitter(TextSplitter):
    """
    Implementation of splitting text that looks at characters.
    该类是对TextSplitter类的扩展，用于根据字符来分割文本。
    """
    def __init__(self, separator: str = "\n\n", **kwargs: Any):
        """
        Create a new TextSplitter.
        创建一个新的TextSplitter实例。
        """
        super().__init__(**kwargs)  # 调用父类的初始化方法
        self._separator = separator  # 将传入的分隔符保存到类的属性中
    def split_text(self, text: str) -> List[str]:
        """
        Split incoming text and return chunks.
        将传入的文本进行分割，并返回分割后的片段。
        """
        # First we naively split the large input into a bunch of smaller ones.
        # 首先，我们简单地将大的输入文本分割成许多小的片段
        if self._separator:  # 如果分隔符存在
            splits = text.split(self._separator)  # 使用分隔符对文本进行分割
        else:  # 如果分隔符不存在
            splits = list(text)  # 则直接将文本转为字符列表
        return self._merge_splits(splits, self._separator)  # 合并分割后的片段，并返回结果
```
可以看到，通过自定义分隔符，CharacterTextSplitter可以按分隔符对文本进行分割。一个完整的示例如下：
```python
from langchain.document_loaders import TextLoader
loader = TextLoader("./tsinghua.txt", "utf-8")
pages = loader.load_and_split()
from langchain.text_splitter import CharacterTextSplitter
chunk_size = 60  # 每段字数长度
chunk_overlap = 3  # 重叠的字数
text_splitter = CharacterTextSplitter(chunk_size=chunk_size,
                                     chunk_overlap=chunk_overlap,separator = ". ")  # 注意这里分隔符的设计
split_docs_CTS = text_splitter.split_documents(pages)
print((split_docs_CTS))  # 11
```

读者可以自行尝试。

在实际中，使用 RecursiveCharacterTextSplitter 对文本进行分割更常用，这个类可以根据段落的长度对文本内容进行切分，即通过固定尺寸对文本进行切分。RecursiveCharacter TextSplitter源码如下：
```python
class RecursiveCharacterTextSplitter(TextSplitter):
    """
    Implementation of splitting text that looks at characters.
    Recursively tries to split by different characters to find one
    that works.
    该类是TextSplitter的扩展，专注于字符级别的文本分割。
    它会递归地尝试使用不同的字符进行分割，以找到一个有效的分割方式。
    """
    def __init__(self, separators: Optional[List[str]] = None, **kwargs: Any):
        """
        Create a new TextSplitter.
        创建一个新的TextSplitter实例。
        """
        super().__init__(**kwargs)  # 调用父类的初始化方法
        self._separators = separators or ["\n\n", "\n", " ", ""]  # 设置默认分隔符列表
    def split_text(self, text: str) -> List[str]:
        """
        Split incoming text and return chunks.
        将传入的文本进行分割，并返回分割后的片段。
        """
        final_chunks = []  # 存储最终的分割片段
        # Get appropriate separator to use
        separator = self._separators[-1]  # 设置默认的分隔符为列表中的最后一个
        for _s in self._separators:  # 遍历分隔符列表
            if _s == "":
                separator = _s  # 如果列表中有空字符串，则使用它作为分隔符，并退出循环
                break
            if _s in text:  # 如果当前分隔符在文本中存在，则使用它作为分隔符，并退出循环
                separator = _s
                break
        # 使用选择的分隔符对文本进行分割
        if separator:  # 如果分隔符存在
            splits = text.split(separator)  # 使用分隔符对文本进行分割
        else:  # 如果分隔符不存在（即为空字符串）
            splits = list(text)  # 则直接将文本转换为字符列表
        # Now go merging things, recursively splitting longer texts.
        _good_splits = []  # 存储长度小于chunk_size的片段
        for s in splits:  # 遍历分割后的片段
            if len(s) - self._length_function(s) < self.chunk_size:  # 如果片段的长度小于chunk_size
                _good_splits.append(s)  # 将该片段添加到_good_splits列表中
            else:  # 如果片段的长度大于或等于chunk_size
                if _good_splits:  # 如果_good_splits列表中已有内容
                    merged_text = self._merge_splits(_good_splits, separator)  # 合并列表中的内容
                    final_chunks.extend(merged_text)  # 将合并后的内容添加到final_chunks列表中
                    _good_splits = []  # 清空_good_splits列表
                other_info = self.split_text(s)  # 对当前片段进行递归分割
                final_chunks.extend(other_info)  # 将递归分割后的内容添加到final_chunks列表中
        if _good_splits:  # 处理剩余的片段（如果有）
            # 合并剩余的片段
            merged_text = self._merge_splits(_good_splits, separator)
            # 将合并后的内容添加到final_chunks列表中
            final_chunks.extend(merged_text)
        return final_chunks  # 返回最终的分割结果
```
下面我们创建一个 RecursiveCharacterTextSplitter 实例，配置 chunk_size 值为5、chunk_overlap值为3。我们的方法是通过length函数来度量每个块的字符数，代码如下：
```python
from langchain.document_loaders import TextLoader
loader = TextLoader("./tsinghua.txt", "utf-8")
pages = loader.load_and_split()
from langchain.text_splitter import RecursiveCharacterTextSplitter
chunk_size = 5  # 每段字数长度
chunk_overlap = 3  # 重叠的字数，注意人工设置了分隔符后，这里的参数不起作用
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap,separators = ["\n\n", "\n", " ", "."])
split_docs_CTS = text_splitter.split_documents(pages)
print((split_docs_CTS))  # 11
```

可以看到，由于我们在分隔类中设置了 separators 分隔符，因此段落首先会根据分隔符进行分割。而我们设置的 chunk_size 与 chunk_overlap 参数，仅在分隔符默认的情况下才会使用，这一点请读者注意。

这样做的原因是：对于文本的内容读取，LLM一般会限制上下文窗口的大小，比如4k、16k、32k等。简单地说，我们需要对大文本进行文本分割处理。常用的文本分割器是RecursiveCharacterTextSplitter。在使用时，我们可以通过separators参数指定分隔符来分割文本。如果没有指定分隔符，那么该分割器会根据默认设置或人为设定的文本分割长度，将长文本分割成若干个指定长度的短文本。

在具体使用上，文本分割主要有两个考虑：

（1）将语义相关的句子放在一起形成一个chunk。一般根据不同的文档类型定义不同的分隔符，或者可以选择通过模型进行分割。

（2）chunk控制为固定的大小，可以通过函数去计算。默认通过len函数计算，模型内部一般使用token进行计算。token通常指的是将文本或序列数据划分成更小的单元或符号，便于机器理解和处理。

因此，在具体使用上，我们可以根据需要，采用不同的分块方式来完成文本的分割。

# 6.2.3 基于LangChain的文本分块

下面我们需要完成对文本分块向量转换，可以使用文本分块方法来获取的文本进行分块，并把每个分块内容转换为向量数据。

需要注意，相对于前面对分块进行向量转换的方式，在这里使用了并发向量转换函数from_adocuments。完整实现代码如下：
```python
document = context
from langchain.docstore.document import Document # 导入Document类
# 创建一个Document对象，包含处理后的文档内容和来源信息
document = [Document(page_content=document, metadata={"source": target_file})]
from langchain.text_splitter import RecursiveCharacterTextSplitter
# 导入RecursiveCharacterTextSplitter类
chunk_size = 1280 # 定义每段字数长度
chunk_overlap = 128 # 定义重叠的字数
# 创建一个用于分割段落的RecursiveCharacterTextSplitter对象
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap)
documents = text_splitter.split_documents(document) # 对文档进行分块
print(len(documents))
```
由于我们采用了并发的形式进行计算，而没有确定计算顺序，因此在返回时，既返回了计算后的向量，也返回了改变了顺序的文本内容。结果如图6-8所示。

图6-8 返回的结果值

<结果值相关内容因图片模糊无法准确识别>

从上面输出结果中，我们可以清晰地看到所需查询的关键词对应的内容，这表明我们所采用的搜索方法是行之有效的。


![image](https://github.com/user-attachments/assets/ad7692cc-d5e4-40f1-95c1-47f1be50512e)


# 6.2.4 找到最近似问题的文本段落
在完成了上一小节的文本分块后，下一步的目标是基于已有的内容完成文本问答。在这里，我们可以使用一个新的Prompt来完成文本问答，也可以使用前面学习过的内容来实现文本问答。代码如下：
```python
target_file = "./alltxt/2022-03-31_中国工商银行股份有限公司_601398_工商银行2021年_年度报告.txt"
def get_single_jsonFile(file_path):
    import json
    context_list = []
    # 假设TXT文件的内容是这样的，每一行都是一个json对象
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        for line in lines:
            # 将每一行的字符串转换为字典
            data = json.loads(line)
            # 提取并打印'inside'字段的值
            _line = (data.get('type') + "_" + data.get('inside'))
            context_list.append(_line)
    return context_list
context_list = get_single_jsonFile(target_file)
context = "".join(context_list)
document = context
from langchain.docstore.document import Document # 导入Document类
# 创建一个Document对象，包含处理后的文档内容和来源信息
document = [Document(page_content=document, metadata={"source": target_file})]
from langchain.text_splitter import RecursiveCharacterTextSplitter
chunk_size = 1280 # 定义每段字数长度
chunk_overlap = 128 # 定义重叠的字数
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap)
documents = text_splitter.split_documents(document) # 对文档进行分块
documents = [doc.page_content for doc in documents]
from rank_bm25 import BM25Okapi
bm25 = BM25Okapi(documents)
query = "2021年工商银行境内优先股工行优1的股息是多少？"
# 使用BM25模型查找最相似的文本
scores = bm25.get_scores(query)
import numpy as np
most_similar_index = np.where(scores == max(scores))[0][0]#scores.index(max(scores))
most_similar_text = documents[most_similar_index]
print(f"Query: {query}")
print(f"Most similar text: {most_similar_text}")
```
图6-9 使用近似查找的目标段落

<相关内容因图片模糊无法准确识别>

可以看到，此时通过比对最近似的文本段落内容，我们获取到BM25匹配的最合适的段落。下一步，我们需要将它送入LLM终端进行文本问答。


![image](https://github.com/user-attachments/assets/b15e28e8-7f99-4672-9d7e-7bd056a728ea)


# 6.2.5 使用LLM终端完成智能文本问答
在完成了前期的向量查询后，下一步的目标是基于已有的内容完成文本问答。在这里我们可以组建一个新的模板，提供相应的问题并找到对应的目标，从而完成智能文本问答。完整代码如下：
```python
target_file = "./alltxt/2022-03-31_中国工商银行股份有限公司_601398_工商银行2021年_年度报告.txt"
def get_single_jsonFile(file_path):
    import json
    context_list = []
    # 假设TXT文件的内容是这样的，每一行都是一个json对象
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        for line in lines:
            # 将每一行的字符串转换为字典
            data = json.loads(line)
            # 提取并打印'inside'字段的值
            _line = (data.get('type') + "_" + data.get('inside'))
            context_list.append(_line)
    return context_list
context_list = get_single_jsonFile(target_file)
context = "".join(context_list)
document = context
from langchain.docstore.document import Document # 导入Document类
# 创建一个Document对象，包含处理后的文档内容和来源信息
document = [Document(page_content=document, metadata={"source": target_file})]
from langchain.text_splitter import RecursiveCharacterTextSplitter
chunk_size = 1280 # 定义每段字数长度
chunk_overlap = 128 # 定义重叠的字数
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap)
documents = text_splitter.split_documents(document) # 对文档进行分块
documents = [doc.page_content for doc in documents]
from rank_bm25 import BM25Okapi
bm25 = BM25Okapi(documents)
query = "2021年工商银行境内优先股工行优1的股息是多少？"
# 使用BM25模型查找最相似的文本
scores = bm25.get_scores(query)
import numpy as np
most_similar_index = np.where(scores == max(scores))[0][0]#scores.index(max(scores))
most_similar_text = documents[most_similar_index]
print(f"Query: {query}")
print(f"Most similar text: {most_similar_text}")
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["query", "documen"],
    template="""作为一名专业财务检索专家，现在传递给你一个检索信息(query)，你会参考对应的文本材料(documen)，一步一步地仔细思考，按步骤和要求完成任务。
"""
)
from 新第四章_Langchain知识图谱 import llm_chatglm
# 这是一个用于生成剧情梗概的LLMChain
llm = llm_chatglm.ChatGLM()
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = (chain.run({"query": query,"documen":most_similar_text}))
print(result)
```
从上面代码可以看到，此时我们串联了一整套的数据生成流程，根据各个不同阶段使用不同方法，协同了BM25和LLM终端，最终实现了智能问答。
# 6.3 使用LLM终端完成反向问题推断

在上一节中，我们完成了智能问答应用。从整体上来看，基于LLM的应用开发遵循同一个开发流程，即先提出问题，再查找答案，最后解决问题。下面先回顾一下我们如何基于ChatGLM完成智能问答，其中一个中心思路是“先检索再整合”，大致步骤如下：

步骤01先准备好文档，把每个文档切成若干个小的模块。

步骤02当用户发来一个问题的时候，在多个小的文本模块中进行检索，得到相关性最高的一个模块。

步骤03将问题和检索结果合并，重写为一个请求发给LLM终端进行问答。

这3个步骤实际完成的工作是将用户请求的query和document做匹配，也就是所谓的问题-文档匹配。问题-文档匹配的困难在于问题和文档各自的表达方式存在较大差异。通常query以疑问为主，而document则以陈述说明



为主，这种差异可能会影响最终匹配的效果。一种改进的方法是跳过问题和文档匹配部分，先通过document生成一批候选的问题-答案匹配，当用户发来请求的时候，直接将query和候选的question做匹配，进而找到相关的document片段。此时的具体思路如下：
- 首先准备好文档，并整理为纯文本的格式，把每个文档切成若干个小的模块。
- 然后调用LLM终端，根据每个模块生成若干个候选的question。
- 最后将问题和答案合并，重写为一个新的请求发给ChatGLM进行问答。
### 6.3.1 文本问题提取实战
按照本节开头的分析，首先就是完成问题的提取工作。在此，为了更好地匹配文本内容与所属目标，我们使用如下的Prompt：
```python
prompt = PromptTemplate(
    input_variables=["document", "file_name"],
    template="""作为一名专业财务检索专家，现在传递给你一套参考文本材料{document}，你的工作是根据参考文本材料内容，以及文本材料的文件名称{file_name}，
设计出最多2个涉及当前文本的问题，一步一步地仔细思考，按步骤和要求完成任务。
下面是几个要求：
1. 问题要结合文件名称中具体的公司名称和年度来设计，并要以具体的公司名称开头；
2. 你所提出的问题必须能在文本材料中找到具体的答案，如果找不到就不要设计这个问题；
3. 只需要设计出2个问题，并且一定要带有答案，问题和答案均来自给你的文本。
你在回答时，需要将问题和答案在同一个序列中生成，并且要回答对应的答案，你可以参考如下的示例：
生成结果的少量示例如下：
["问：2018年国泰平安衍生产品资产是如何计算的？答：根据材料显示衍生产品资产计算方式如下。",
"问：2020年江苏中铁的权重估值是多少？答：根据材料显示，风险权重估值为2.56。",
"问：1997年华南之星的违约金是多少？答：根据材料显示华南之星的违约金是500元整。"]
你需要严格按照生成答案示例进行回复，生成回答前要一步一步地思考，一定要在给你的参考文本文件中找到对应的答案并回答。
如果没有确定的答案，就不要生成问题。
"""
)
```
在上面代码中，我们详细设计了一整套Prompt提示模板，从而明确地告诉LLM终端如何去设计模型，并着重强调设计的问题必须能在文本中找到答案。完整的程序代码如下：
```python
target_file = "./alltxt/2022-03-31_中国工商银行股份有限公司_601398_工商银行2021年_年度报告.txt"
def get_single_jsonFile(file_path):
    import json
    context_list = []
    # 假设TXT文件的内容是这样的，每一行都是一个json对象
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        for line in lines:
            # 将每一行的字符串转换为字典
            data = json.loads(line)
            # 提取并打印'inside'字段的值
            _line = (data.get('type') + "_" + data.get('inside'))
            context_list.append(_line)
    return context_list
context_list = get_single_jsonFile(target_file)
context = "".join(context_list)
document = context
from langchain.docstore.document import Document # 导入Document类
# 创建一个Document对象，包含处理后的文档内容和来源信息
document = [Document(page_content=document, metadata={"source": target_file})]
from langchain.text_splitter import RecursiveCharacterTextSplitter
chunk_size = 1280 # 定义每段字数长度
chunk_overlap = 128 # 定义重叠的字数
# 创建一个RecursiveCharacterTextSplitter对象
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap)
documents = text_splitter.split_documents(document) # 对文档进行分块
documents = [doc.page_content for doc in documents]
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["document", "file_name"],
    template="""作为一名专业财务检索专家，现在传递给你一套参考文本材料{document}，你的工作是根据参考文本材料内容，以及文本材料的文件名称{file_name}，
设计出最多2个涉及当前文本的问题，一步一步地仔细思考，按步骤和要求完成任务。
下面是几个要求：
1. 问题要结合文件名称中具体的公司名称和年度来设计，并要以具体的公司名称开头；
2. 你所提出的问题必须能在文本材料中找到具体的答案，如果找不到就不要设计这个问题；
3. 只需要设计出2个问题，并且一定要带有答案，问题和答案均来自给你的文本。
你在回答时，需要将问题和答案在同一个序列中生成，并且要回答对应的答案，你可以参考如下的示例：
生成结果的少量示例如下：
["问：2018年国泰平安衍生产品资产是如何计算的？答：根据材料显示衍生产品资产计算方式如下。",
"问：2020年江苏中铁的权重估值是多少？答：根据材料显示，风险权重估值为2.56。",
"问：1997年华南之星的违约金是多少？答：根据材料显示华南之星的违约金是500元整。"]
你需要严格按照生成答案示例进行回复，生成回答前要一步一步地思考，一定要在给你的参考文本文件中找到对应的答案并回答。
如果没有确定的答案，就不要生成问题。
"""
)
from 新第四章_Langchain知识图谱 import llm_chatglm
# 这是一个用于生成剧情梗概的LLMChain
llm = llm_chatglm.ChatGLM()
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
document = documents[-1]
result = (chain.run({"document":document,"file_name":target_file}))
print(result)
``` 
