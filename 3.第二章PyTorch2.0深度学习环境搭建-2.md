### 第2章 PyTorch 2.0深度学习环境搭建
NVIDIA cuDNN是一个GPU加速的深度神经网络基元库。
- 下载cuDNN v8.9.4 (2023年8月8日)，适用于CUDA 12.x
- 下载cuDNN v8.9.4 (2023年8月8日)，适用于CUDA 11.x

![image](https://github.com/user-attachments/assets/fa1357d8-0849-43a3-b8c8-8ca7ab0bad1b)


适用于Windows和Linux、Ubuntu（x86_64、arm64）的本地安装程序
- Windows本地安装程序（Zip）
- Linux x86_64的本地安装程序（Tar）
- Linux PPC的本地安装程序（Tar）
- Linux SBSA本地安装程序（Tar）

![image](https://github.com/user-attachments/assets/5281398a-dc22-483f-854d-b244d96f45fe)


#### （3）

下载的cuDNN是一个压缩文件，将它解压并把所有的目录复制到CUDA安装主目录中（直接覆盖原来的目录），CUDA安装主目录如下：

|名称|修改日期|类型|大小|
| ---- | ---- | ---- | ---- |
|bin|2021/8/6 16:27|文件夹| |
|compute - sanitizer|2021/8/6 16:26|文件夹| |
|extras|2021/8/6 16:26|文件夹| |
|include|2021/8/6 16:27|文件夹| |
|lib|2021/8/6 16:26|文件夹| |
|libnvptx|2021/8/6 16:26|文件夹| |
|nvml|2021/8/6 16:26|文件夹| |
|nvm|2021/8/6 16:26|文件夹| |
|src|2021/8/6 16:26|文件夹| |
|tools|2021/8/6 16:26|文件夹| |
|CUDA_Toolkit_Release_Notes|2020/9/16 13:05|TXT文件|16 KB|
|DOCS|2020/9/16 13:05|文件夹|1 KB|
|EULA|2021/4/14 21:54|TXT文件|61 KB|
|NVIDIA_SLA_cuDNN_Support|2021/4/14 21:54|TXT文件|23 KB|

#### （4）
确认PATH环境变量，这里需要将CUDA的运行路径加载到环境变量的PATH路径中。安装CUDA时，安装向导能自动加入这个环境变量值，确认一下即可。

#### （5）
最后完成PyTorch 2.0.1 GPU版本的安装，只需在终端窗口中执行本小节开始给出的PyTorch安装命令即可。
```bash
# CUDA 11.8
conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia
```

![image](https://github.com/user-attachments/assets/2e5b5331-639a-4c98-a32f-5f81deb7e4ae)


### 2.2.3 Hello PyTorch

![image](https://github.com/user-attachments/assets/e66a2873-39d0-4644-be4f-bc596de0a33b)


到这里我们已经完成了PyTorch 2.0的安装。下面使用PyTorch 2.0做一个小练习。打开CMD，依次输入如下命令，验证安装是否成功。
```python
import torch
result = torch.tensor(1) + torch.tensor(2.0)
result
```
结果如下：
```
Python 3.11.4, packaged by Anaconda, Inc. (main, Jul 5 2023, 13:47:18)
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> result = torch.tensor(1) + torch.tensor(2.)
>>> result
```
或者打开前面安装的PyCharm IDE，先新建一个项目，再新建一个hello_pytorch.py文件，输入如下代码：
```python
import torch
result = torch.tensor(1) + torch.tensor(2.0)
print(result)
```
最终结果请读者自行验证。

### 2.3 Hello ChatGLM3
ChatGLM系列是智谱AI发布的一系列大语言模型，因为其优秀的性能和良好的开源协议，在国产大模型和全球大模型领域都有很高的知名度。其开源的第三代基座大语言模型ChatGLM3-6B的性能较前一代有了大幅提升，可以认为是十分强大的中文基础大模型。

#### 2.3.1 ChatGLM3简介与安装

自诞生以来，ChatGLM便以其卓越的性能和广泛的应用而闻名于世，成为国产大语言模型中最强大、最著名的模型。2023年3月，第一代ChatGLM-6B面世，开源之后便获得了广泛的关注和使用。短短3个月后，ChatGLM2的发布再次引发了科技界的热议。2023年10月27日，智谱AI再次发布了第三代基础大语言模型ChatGLM3系列，这一系列共包含了3个模型：基础大语言模型ChatGLM3-6B-Base、对话调优大语言模型ChatGLM3-6B和长文本对话大语言模型ChatGLM3-6B-32K。

1. **ChatGLM3简介**

ChatGLM3的发布，标志着中国在自然语言处理领域的研究取得了新的突破。这一系列模型不仅具备了更强大的语言理解能力，更能在不同场景下进行高效、精准的应用。其中，ChatGLM3-6B-Base作为基础大语言模型，凭借其出色的性能表现，成为众多领域中的得力助手；ChatGLM3-6B通过对话调优，实现了更加智能、自然的交互体验；ChatGLM3-6B-32K作为长文本对话大语言模型，以其在长文本处理方面的卓越表现，广泛应用于新闻媒体、科技文献等领域。

ChatGLM3模型说明如下表：

|版本模型|模型介绍|上下文长度|备注|
| ---- | ---- | ---- | ---- |
|ChatGLM3-6B-Base|基础大语言模型，预训练结果|8K|基础模型|
|ChatGLM3-6B|基础大语言模型，针对对话微调调优，适合对话|8K| |
|ChatGLM3-6B-32K|对话调优的大语言模型，但是支持32K上下文|32K|不支持工具函数调用|


ChatGLM系列的持续升级和优化，不仅提升了国产大语言模型的国际竞争力，更为人工智能领域的发展注入了新的活力。未来，我们有理由相信，ChatGLM将继续秉持开放、共享的精神，推动人工智能技术的创新和发展，为人类社会的进步贡献更多的力量。

需要注意，ChatGLM3的功能不局限于生成对话，它在工具调优、Prompt（提示）调优、代码执行等方面都有很大提升。具体来说，ChatGLM3的功能提升主要集中在以下几点：

- **更强大的基础模型**：ChatGLM3-6B的基础模型ChatGLM3-6B-Base采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base在10B以下的预训练模型中具有最强的性能。

- **更完整的功能支持**：ChatGLM3-6B采用了全新设计的Prompt格式，除正常的多轮对话外，同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和Agent任务等复杂场景。

- **更全面的开源序列**：除了对话模型ChatGLM3-6B外，还开源了基础模型ChatGLM-6B-Base、长文本对话模型ChatGLM3-6B-32K。以上所有模型的源码权重对学术研究完全开放，在填写问卷进行登记后还可免费商业使用。

2. **ChatGLM3安装**

在介绍完ChatGLM3激动人心的新功能之后，下面开始使用ChatGLM3，读者有两种方式获取ChatGLM3。

- **第一种方式**：是从GitHub上获取ChatGLM3源码（笔者不推荐）。读者可以登录GitHub，搜索关键字“ChatGLM”，打开对应的链接。这里读者可以使用GitHub自行下载对应的源文件，但需注意，这里的源码权重并不直接提供，而是需要单独下载。

![image](https://github.com/user-attachments/assets/8c1574e3-ef48-43f9-aa50-42a37837c78a)

![image](https://github.com/user-attachments/assets/53e55497-5a6b-4db7-a667-1993b6b9bbdb)


- **第二种方式**：是通过魔塔社区（ModelScope）（笔者推荐）来完成ChatGLM3的配置和使用。魔塔社区的库包可以使用pip命令进行安装，如下所示。

```bash

pip install modelscope
```
待安装结束后，即可在PyCharm中新建一个空的项目，通过在代码中调用ChatGLM3-6B模型来生成对话。

![image](https://github.com/user-attachments/assets/50f40937-1ddc-4887-a734-7c140b52d3a1)


### 2.3.2 CPU版本的ChatGLM3推演
我们首先完成CPU版本的ChatGLM3的推演。由于采用CPU进行推演，因此在自动下载完模型参数后，推演的耗时较长，这一点请读者注意。
```python
import torch
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = snapshot_download("ZhipuAI/chatglm3-6b", revision = "v1.0.0")
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

with torch.no_grad():
    model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True).cpu().float()

    model = model.eval()
    response, history = model.chat(tokenizer, "你好", history=[])
    print(response)
    response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
    print(response)
```
由于第一次调用，需要从网上直接下载对应的权重文件，这个权重文件比较大，因此读者需要根据自身的网络带宽情况等待文件下载结束。

下载过程如下：

![image](https://github.com/user-attachments/assets/7b213835-7509-492b-ad4a-b70d83a89b42)


如果有报错，可以等待1分钟后重新连接，再进行下载。

全部下载完成后，ChatGLM3自动进入运行模式，在代码中可以设置需要让模型回答的普通问题，上述代码段的回答结果如下所示。注意：返回回答结果的时间会长一点，请读者耐心等待。
```
你好！我是人工智能助手ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。
晚上睡不着的原因有很多，比如压力、焦虑、饮食、生活习惯等。这里给你提供一些建议，帮助你更容易入睡：
1. 保持规律作息：每天尽量在相同的时间入睡和起床，以帮助调整你的生物钟。
2. 创造一个良好的睡眠环境：保持房间安静、舒适、黑暗，并保持适宜的温度。
3. 避免刺激性活动：在睡前一小时内避免从事刺激性强的活动，如剧烈运动、看恐怖电影等。
4. 放松身心：可以尝试一些放松身心的方法，如深呼吸、瑜伽、冥想等。
5. 避免过多使用电子产品：睡前一小时内避免使用手机、电脑等电子产品，以减少蓝光的影响。
6. 适量饮水：晚上适量饮水可以防止夜间因口渴而醒来。
```
请读者自行运行代码进行验证。需要读者注意的是，即使问题是一样的，但是每一次运行代码得到的回答也有可能不一样。因为我们所使用的ChatGLM是生成式模型，前面的生成会直接影响后面的生成，而这点也是生成模型相对于一般模型不同的地方。前面的结果有了波动，后面就会发生很大的变化，会有一个滚雪球效应。

另外，CPU版本的ChatGLM3推演耗时较长，因此推荐读者尽量采用GPU版本的模型进行后续的学习。

![image](https://github.com/user-attachments/assets/1c98b13b-4c15-4f22-932b-219ed0850424)


### 2.3.3 GPU（INT4或INT8量化）版本的ChatGLM3推演
在PyTorch中，模型量化是一种优化技术，它可以将模型的权重和激活值从浮点数转换为低精度的整数格式，如INT4和INT8。这种转换可以显著减少模型的大小和计算复杂度，同时加快推理速度，特别是在资源受限的设备上，如智能手机、嵌入式系统和物联网设备等。现在，我们来详细了解一下INT4和INT8这两种GPU模型量化推演方式。

1. **INT8量化**

INT8量化是将模型的权重和激活值从浮点数转换为8位整数的过程。与浮点数相比，虽然INT8整数表示的数值范围较小，精度较低，但它可以显著减少存储和计算的需求。

在INT8量化中，模型的权重和激活值会经过一个量化过程，该过程包括缩放和偏移，以确保量化后的值能够尽可能地保留原始浮点数的信息。在推理时，这些量化值会被反量化回浮点数进行计算，然后再量化回INT8进行下一步操作。

2. **INT4量化**

INT4量化是一种更为激进的量化方式，它将模型的权重和激活值量化为4位整数。由于表示范围更小，精度更低，INT4量化通常会导致更大的精度损失。然而，与INT8相比，INT4量化可以进一步减少模型的存储需求和计算复杂度。

需要注意，INT4量化在实际应用中相对较少见，因为过低的精度可能导致模型性能显著下降。此外，不是所有的硬件都支持INT4操作，因此在选择量化方式时需要考虑硬件的兼容性。

3. **量化的优势**

量化具有以下优势：

- **减少模型大小**：量化可以显著减少模型的存储需求，使得部署更加轻便。

- **提高推理速度**：低精度的计算通常比浮点数计算更快。

- **降低能耗**：在移动和嵌入式设备上，低能耗是至关重要的，量化有助于减少计算时的能耗。

4. **注意事项**

量化的注意事项如下：

- **精度损失**：量化通常会导致一定程度的精度损失，需要在性能和精度之间进行权衡。

- **模型需重新训练**：有些量化方法可能需要重新训练或微调模型以恢复性能。 

- **硬件支持**：不同的硬件对量化的支持程度不同，需要确保所选的量化方式与目标硬件兼容。



下面来完成INT4量化的ChatGLM3推演代码，如下所示（注意model的构建方式）：

```python

import torch
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = snapshot_download("ZhipuAI/chatglm3-6b", revision = "v1.0.0")
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

with torch.no_grad():
    # 采用量化方案的ChatGLM3
    model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True).quantize(4).cuda()

    model = model.eval()
    response, history = model.chat(tokenizer, "你好", history=[])
    print(response)
    response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
    print(response)
```
请读者自行打印结果进行验证和比较。

如果想换成INT8格式的模型构建方法，则可以采用如下的model构建：
```python
model = AutoModel.from_pretrained(model_dir,
                                  trust_remote_code=True).quantize(8).cuda()
```

可以看到，只需简单地将quantize(4)中的数字4改成8即可。具体请读者自行验证。

### 2.3.4 GPU（half或float量化）版本的ChatGLM3推演

使用GPU对ChatGLM3进行推演，除了前面提到量化方法之外，常规方案的GPU推演中half和float是两种最常用的格式。它们分别对应16位浮点数（float16）和32位浮点数（float32）。half格式占用13GB显存，float格式占用40GB显存。下面来具体介绍。

1. **half格式**

half格式即16位浮点数（float16）。与float相比，half格式占用的内存减半，这在大规模深度学习应用中具有显著优势。它允许我们在相同的GPU内存限制下加载更大规模的模型或处理更多数据。此外，随着现代GPU硬件对float16操作的支持不断增强，使用half格式还可能带来计算速度的提升。然而，half格式有一个固有的缺点，即降低精度，这可能导致在某些情况下出现数值不稳定或精度损失的情况。

2. **float格式**

接下来谈谈float格式。这种格式提供了较高的精度，能够准确表示大范围的数值。在进行复杂的数学运算或需要高精度结果的场景中，float格式是首选。然而，高精度也意味着更多的内存占用和可能更长的计算时间。对于大规模深度学习模型，尤其是当模型参数众多、数据量巨大时，float格式可能会导致GPU内存不足或推演速度下降的情况。

选择是使用float还是half格式时，需要考虑以下几个因素：

- **模型要求**：某些模型对精度非常敏感，可能需要使用float32。

- **硬件支持**：不是所有GPU都支持float16操作，或者支持的程度不同。 

- **内存限制**：如果GPU内存有限，使用float16可以显著减少内存占用。

- **性能要求**：对于需要快速推演的应用，float16可能是一个更好的选择。



且GPU内存充足，那么float格式可能是更好的选择；如果面临内存限制或希望加快推演速度，那么使用half格式或混合精度训练可能会带来显著的好处。对于ChatGLM3这样的大型模型，混合精度推演可能是一个折中的好选择，它能够在保持一定精度的同时，充分利用GPU的计算能力。

下面是使用half格式完成ChatGLM3模型推演的示例，代码如下：

![image](https://github.com/user-attachments/assets/44732c94-c9a2-4da7-bc4b-261d83babb00)


```python

import torch
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = snapshot_download("ZhipuAI/chatglm3-6b", revision = "v1.0.0")
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

with torch.no_grad():
    # 采用GPU half方案的ChatGLM3
    model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True).half().cuda()

    model = model.eval()
    response, history = model.chat(tokenizer, "你好", history=[])
    print(response)
    response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
    print(response)
```
如果想要使用float格式下的GPU推演模式，则只需要将model的构建代码改为如下方式：
```python
model = AutoModel.from_pretrained(model_dir,
                                  trust_remote_code=True).float().cuda()
```

此时，对于普通用户来说应该会产生一个爆显存的提示。因此在具体选择上，读者应该遵循自己所使用的硬件资源合理选择不同的模型。如果GPU具有较大的显存容量，那么使用half版本的模型可能不是问题；对于显存有限的情况，选择量化版本将是一个明智的决策。

为了保证ChatGLM3模型在后续学习过程中的效率和准确率之间的平衡，笔者将主要使用GPU的half版本来进行讲解。half版本通过降低数据精度来减少显存占用，同时在一定程度上保留了模型的准确性。这种折中方案使得更多用户能够在有限的硬件资源上运行和实验ChatGLM3模型。

在具体学习过程中，读者还应该根据自己的具体条件和需求来调整模型的构建方式，包括修改模型的层数、神经元数量、激活函数等。通过合理调整这些参数，读者可以获得一个既高效又准确的推断结果。

总之，读者在使用ChatGLM3模型时，应该充分了解自己的硬件资源限制，并根据实际情况选择合适的模型版本和构建方式。这样做不仅可以避免显存溢出等问题，还可以确保模型在推演过程中保持高效和准确。

### 2.3.5 离线状态的ChatGLM3的使用
对于部分有特殊需求的读者来说，一个能够离线使用或者部署在私有云架构上的大模型是必不可少的。ChatGLM3也提供了完全离线的ChatGLM部署和使用方法。首先，需要打开魔塔社区在本地的下载地址，一般这个存储地址为：
```
C:\Users\你当前登录用户名\.cache\modelscope\hub\ZhipuAI
```

此时，在这个文件夹下，我们可以看到从魔塔社区下载的全部ChatGLM3的源码与权重文件，局部界面如下所示。

![image](https://github.com/user-attachments/assets/fc403cd0-302c-439a-a923-539642ac7d49)


|名称|修改日期|
| ---- | ---- |
|md|2023/11/5 9:24|
|mmc|2023/11/5 10:03|
|config|2023/11/6 9:04|
|configuration|2023/11/5 9:04|
|configuration_chatglm|2023/11/5 9:04|
|MODEL_LICENSE|2023/11/5 9:04|
|modeling_chatglm|2023/11/5 9:04|
|pytorch_model.bin.index|2023/11/5 10:03|
|pytorch_model-00001-of-00007.bin|2023/11/5 9:12|
|pytorch_model-00002-of-00007.bin|2023/11/5 9:32| 


### 2.3.5 离线状态的ChatGLM3的使用（续）
要离线使用ChatGLM3，读者可以将chatglm3-6b目录直接复制到PyCharm的当前项目目录下，如图2-25所示。然后在代码中修改模型存档地址。

![把ChatGLM3源码放到当前项目目录下](图2-25)

![image](https://github.com/user-attachments/assets/5749a4ca-2b14-47e3-91e3-51c1f354ca80)


对于新的ChatGLM3模型的调用，也可以简单地使用如下代码完成：
```python
# model_dir = snapshot_download("./chatglm3-6b", revision = "v1.0.0") # 注销联网验证代码
model_dir = "../chatglm3-6b"  # 直接提供ChatGLM3的存储地址
...
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir,
                                  trust_remote_code=True).half().cuda()
```
在上面代码中，我们注释了原始的snapshot_download下载和验证函数，替代为直接使用本地模型，相对于当前demo位置的源码和权重存储相对地址。


最终结果请读者自行打印完成。

### 2.3.6 ChatGLM的高级使用
前面我们进行了一次非常小的演示，介绍了ChatGLM的使用。除此之外，ChatGLM还可以有更多用法，例如进行文本内容的抽取，读者可以尝试如下任务：
```python
content="""ChatGLM3-6B是一个最强开源的、支持中英双语的大语言模型，
基于General Language Model (GLM)架构，具有62亿参数。
手机号18888888888
结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。
ChatGLM3-6B使用了较ChatGPT更为高级的技术，针对中文问答和对话进行了优化。
邮箱123456789@qq.com
经过约1T标识符的中英双语训练，辅以监督微调、前缀自助、人类前馈强化学习等技术的加持，
账号:root 密码:xiaohua123
62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答，更多信息请参考我们的博客。
"""
Prompt='从上文中，提取"信息"(keyword,content)，包括:"手机号"、"邮箱"、"账号"、"密码"类型的实体，只输出这些实体，不要其他的。输出JSON格式内容'
input = '{\n\n{}\n\n}'.format(content,Prompt)
print(input)
response, history = model.chat(tokenizer, input, history=[])
print(response)
```
这是一个经典的文本抽取任务，希望通过ChatGLM抽取其中的内容。这里使用了一个Prompt（中文暂时称为“提示”），它是研究者们为了下游任务设计出来的一种输入形式或模板，能够帮助ChatGLM“回忆”起自己在预训练时“学习”到的东西。
Prompt也可以帮助使用者更好地“提示”预训练模型所需要去做的任务，这里通过Prompt的方式向ChatGLM传达了一个下游任务目标，即需要对文本进行信息抽取，抽取其中蕴含的手机号、邮箱、账号密码等常用信息。最终显式结果如图2-26所示。

![文本信息抽取结果](图2-26)
```
{
    "手机号": "18888888888",
    "邮箱": "123456789@qq.com",
    "账号": "root",
    "密码": "xiaohua123"
}
```

![image](https://github.com/user-attachments/assets/a8d2e641-2618-4105-aca5-6c140d7a7f16)


可以看到，这是一个使用JSON表示的抽取结果，其中的内容还根据Prompt中定义的键值，直接抽取了对应的内容。


除此之外，我们还可以使用ChatGLM进行一些常识性的文本问答和编写一定量的代码。当然，要完成这些内容，还需要设定好特定的Prompt，从而使得ChatGLM能更好地理解用户所提出的问题和所要表达的意思。

### 2.4 本章小结
在本章中，首先详细介绍了PyTorch 2.0框架、开发环境和第三方软件的安装：先讲解了PyTorch的独特性和其在机器学习领域的重要地位，然后逐步展开，详细阐述了如何进行环境配置和软件安装，并通过一个非常简单的示例验证了安装环境。

在接下来的代码演示部分，我们以ChatGLM3为例，向读者展示了大语言模型的基本使用方法。ChatGLM3是一种基于Transformer的大语言模型，具有进行自然语言对话和生成连贯文本的能力。我们详细介绍了如何使用ChatGLM3进行对话和文本生成，从而为后续的大模型应用提供了坚实的基础。

本章为后续的深度学习应用提供了必要的基础。希望读者能够充分理解和掌握本章的内容，为后续的学习和实践打下坚实的基础。 
