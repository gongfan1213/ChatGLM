我们还可以切换到Overview标签页，这里记录了实验的各种信息，包括swanlab.init中的参数、最终的实验指标、实验状态、训练时长、Git仓库链接、主机名、操作系统、Python版本、硬件配置等，如图3-20所示。

图3-20 Overview页的展示

```
ResNet50 2024-01-09 11:30:36 
Train ResNet50 for cat and dog Classification.
Status: Success
Start Time: 2024/01/09 11:30:36
Duration: less than 1s
Submit Version: v0.1.3
Git Repository: 
Hostname: DESKTOP-48RX6GM
OS: Windows 10 10.0.22000 990
Python: 3.11.4
Python executable: C:\python311\python.exe
System Hardware: CPU Count: 16 | GPU Count: 1 | GPU Type: NVIDIA GeForce RTX 3090

Configuration information
| Config | value |
| ---- | ---- |
| key | resnet50 |
| model | Adam |
| optim | 0.0001 |
| batch_size | 8 |

Summary
| key | value |
| ---- | ---- |
| test_acc | 0.983714 |
| test_loss | 0.0057 |
| train_loss |  |
```

![image](https://github.com/user-attachments/assets/3073795e-46a1-4030-a032-610a9fd37f7a)

可以看到，训练完成后，此时的准确率为98.5%。至此，我们完成了模型的训练和测试，得到了1个表现非常棒的猫狗分类模型，权重保存在checkpoint目录下。
接下来，我们就基于这个训练好的权重，创建1个gradio网页。

# 3.2.5 使用训练好的模型完成gradio可视化图像分类
在上一小节中，我们完成了模型的训练流程，下面我们将基于此完成可视化图像分类。
## 第一步：模型参数的载入
在上一小节我们完成了模型的训练，接下来只需要完成模型参数的载入任务，代码如下：
```python
import torch, torchvision
# 加载与训练中使用的结构相同的模型
def load_model(checkpoint_path, num_classes = 2,device = "cuda"):
    # 加载预训练的ResNet50模型
    model = torchvision.models.resnet50(weights=None)
    in_features = model.fc.in_features
    model.fc = torch.nn.Linear(in_features, num_classes)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()  # 将模型设置在验证模式
    return model
```
## 第二步：预测函数的编写

完成了模型载入后，下面就是编写预测函数。我们只需要按原有的输入形式来完成预测函数的编写即可，代码如下：
```python
import torch, torchvision
# 加载与训练中使用的结构相同的模型
def load_model(checkpoint_path, num_classes = 2,device = "cuda"):
    # 加载预训练的ResNet50模型
    model = torchvision.models.resnet50(weights=None)
    in_features = model.fc.in_features
    model.fc = torch.nn.Linear(in_features, num_classes)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()  # 将模型设置在验证模式
    return model

from torchvision import transforms
def preprocess_image(image):
    image_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image = image_transform(image).unsqueeze(0)
    return image

model = load_model("./checkpoint/latest_checkpoint.pth")
def predict(image):
    classes = {'0': 'cat', '1': 'dog'}  # 根据实际的类别更新或扩展这个字典
    image = preprocess_image(image)  # 使用训练数据集中的图像尺寸
    with torch.no_grad():
        outputs = model(image)
    # 应用softmax函数以获得概率值
    probabilities = torch.nn.functional.softmax(outputs, dim=1).squeeze()
    # 将类标签映射到概率值
    class_probabilities = {classes[str(i)]: float(prob) for i, prob in enumerate(probabilities)}
    return class_probabilities
```
上面代码将模型的预测以及对图像的处理进行整合，从而完成数据的准备工作。
第三步：可视化图像的编写
下面我们继续完成gradio可视化图像的编写，代码如下：
```python
import gradio as gr
# 定义gradio界面
iface = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil"),
    outputs=gr.Label(num_top_classes=2),
    title="Cat vs Dog Classifier",
)

if __name__ == "__main__":
    iface.launch()
```
上面代码实现了图像的读取与预测，选定一幅图像，单击Submit按钮提交上传后，结果如图3-21所示。

![image](https://github.com/user-attachments/assets/33191e21-4794-4529-bc01-0951f605af93)



图3-21 使用gradio对图像进行预测

Cat vs Dog Classifier

（此处有一张猫狗分类预测结果的图片，左侧是猫狗图片，右侧是预测结果显示“cat” 及概率条）

可以很清楚地看到，此时模型对于图片的预测是：猫的可能性为96%，这也很好地展示了训练的结果。
下面回到代码设计中，在前面对gradio进行介绍时，采用的输入格式为显式标注image，而此时我们对于输入框采用的是gr.Image(type="pil")函数，从名称上来看，此时输入框接收的是一个image类型的图像，而其中的参数表示在将图像传递给预测函数之前，图像是如何被转换的。这里使用的格式为pil。具体来说，它有3种转换格式：numpy、pil和filepath。
- numpy：图像会被转换成一个numpy数组。这个数组的形状是（height, width, 3），其中height是图像的高度，width是图像的宽度，3表示图像有3个颜色通道（通常是RGB，即红、绿、蓝）。数组中的值范围是0~255，这代表了每个颜色通道的强度或亮度。例如，值(0, 0, 0)代表黑色，而值（255, 255, 255）代表白色。
- pil：图像会被转换成一个PIL图像对象。PIL是Python中处理图像的一个流行库，它提供了许多用于图像操作的功能。转换后的PIL图像对象，可以直接使用PIL库提供的各种方法进行处理。 
- filepath：传递的是一个字符串路径，该路径指向一个包含图像内容的临时文件。这意味着图像数据并没有被直接转换或加载到内存中，而是保存在硬盘上的一个临时文件中。预测函数可以根据这个路径去读取和处理图像。
此外，如果图像是SVG（可缩放矢量图形）格式，那么type参数会被忽略，并直接返回SVG的文件路径。这可能是因为SVG图像的处理方式与位图（如JPEG、PNG等）不同，因此不需要或不支持上述的转换方式。

# 3.3 基于网页端的ChatGLM3部署和使用
网页版的客户端提供了一种便捷的途径，使我们能够轻松地实现ChatGLM3的连续性部署和使用。通过这种方式，我们可以在处理问题时连续性地解答多个问题，从而提高工作效率和用户体验。

在本节中，首先将学习如何使用gradio搭建一个简单的ChatGLM3服务器。这个过程将涉及gradio的使用技巧。通过逐步的指导和示例，帮助读者掌握搭建服务器的关键步骤，为后续的使用 。 


奠定基础。 
在成功搭建服务器后，将进一步讲解如何使用官方自带的客户端对问题进行解答。这部分内容将重点介绍客户端的功能和使用方法。通过学习和实践，帮助读者将掌握使用官方客户端进行问题解答的技巧和流程。 
通过本章的学习，读者将能够熟练地完成ChatGLM3的网页版部署和使用，从而为以后在实际应用中处理连续性问题提供有力的支持。 

# 3.3.1 使用gradio搭建ChatGLM3网页客户端 

对于使用网页端完成部署的用户来说，最少需要准备一个自定义的网页端界面。在网页端界面上，可以设置文本输入框供用户输入问题或文本，并显示ChatGLM3的回复和相应的提示信息。此外，还可以添加一些额外的功能，如清空输入框、复制回复等。 

以上内容对于有过前端经验的读者来说可能并不复杂，但是对于深度学习模型开发人员来说，从头学习前端知识及其代码编写，可能需要耗费大量的时间和成本，那么有没有一种简易的方法可以帮助我们完成网页客户端的搭建。 

答案当然是有。gradio提供了一个自定义的ChatGLM3网页对话客户端模板，只需要简单的几行代码，即可完成一个私有云ChatGLM3网页客户端。下面是一个简单的使用ChatInterface组件完成对话框搭建的示例，代码如下： 
```python
import gradio as gr
def echo(message, history):
    return message
demo = gr.ChatInterface(fn=echo, examples=["hello", "hola", "merhaba"], title="Echo Bot")
demo.launch()
```
运行代码后，页面如图3-22所示。 

![image](https://github.com/user-attachments/assets/05f64794-6a40-4ce2-b4f6-5d423de3ac60)


图3-22 使用ChatInterface组件构建的对话框

（此处是一个名为Echo Bot的对话框界面示意图，有输入框、提交等按钮及示例词）



这里只是一个简单的页面，即使用固定的格式搭建了一个用作展示的页面，如果需要对问答进行反馈，则需要进一步完成内容的注入。下面是一个使用ChatGLM3完成的对话框示例，代码如下： 
```python
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = "../chatglm3-6b"  # 直接提供ChatGLM3的存储地址
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).quantize(4).cuda()
model = model.eval()

import gradio as gr

def echo(message, history):
    response, _history = model.chat(tokenizer, message, history=[])
    return response

demo = gr.ChatInterface(fn=echo, examples=["hello", "hola", "merhaba"], title="Echo Bot")
demo.launch()
```
具体结果请读者自行尝试。 

# 3.3.2 使用ChatGLM3自带的网页客户端 

![image](https://github.com/user-attachments/assets/7704b67a-a2f2-4501-baf4-b71d5e13fe21)

除了使用自定义的gradio组件完成网页客户端的搭建之外，智谱AI的ChatGLM3在创建之初就本着“方便用户，以人为本”的原则，为用户提供了对应的网页客户端代码，从而方便用户直接使用网页端的ChatGLM3应用程序。 


读者可以直接打开本章配套源码文件夹中的web_demo.py文件。在运行这个文件之前，部分读者可能需要安装一些适配的库包，安装命令如下（推荐使用gradio==3.40.0固定版本）： 
```bash
pip install gradio==3.40.0, mdtex2html
```
注意：在安装的时候，部分读者会有如图3-23所示的报错，这是因为安装的gradio版本不对，按上述命令指定安装gradio==3.40.0即可解决。当有输入无输出反应时，也请读者尝试更新gradio版本，解决输出无结果的问题。 
```
user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Textbox' object has no attribute 'style'. Did you mean: 'scale'?
```
图3-23 读取报错 

接下来，直接在Python中右击run运行web_demo.py文件，在合并了存档记录后，读者的网页客户端会自动打开如下地址： 
```
Running on local URL: http://127.0.0.1:7860
```
此时，ChatGLM3会运行在本地，界面如图3-24所示。 



![image](https://github.com/user-attachments/assets/e8323b80-c69b-4f97-9909-f259ceddfa3a)


图3-24 初始化好的ChatGLM网页客户端

（此处是ChatGLM3-6B网页客户端界面示意图，有输入框、提交按钮、清空历史按钮及Maximum length 、Top P、Temperature调节滑块 ）

在此界面可以开启多轮对话，读者可以依据自己的问题与ChatGLM进行交互，从而获得相关问题的答案。 

# 3.4 基于私有云服务的ChatGLM3部署和使用 

云服务是一种基于互联网的相关服务的使用和交互模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源，用户可以通过网络按需获得所需的计算资源与能力的服务。 

在云服务领域中，API接口是一项至关重要的技术，它实现了应用与计算的有效分离，使得不同平台能够专注于各自擅长的工作。对于自然语言处理模型而言，构建API接口能够显著扩展其应用场景的支持范围。 

基于ChatGLM3模型的API接口私有云服务，为其他应用程序或平台提供了一个便捷的途径，通过调用该接口即可轻松获取ChatGLM3模型的强大功能。通过对外开放API接口，其他程序能够发送自然语言查询请求，并快速获取精准的回复结果。由此，ChatGLM3模型得以被无缝集成至各类应用中，为各种场景提供坚实支持。 

本节除了详细介绍如何利用FastAPI实现私有云服务的构建外，还旨在为读者提供更丰富的学习体验。为此，本节特别呈现了官方推荐的streamlit工具，用于快速搭建大模型对话网页，进一步展示API接口在自然语言处理领域中的广泛应用与深远影响。 

第3章 基于gradio的云上自托管ChatGLM3部署实战 | 73
# 3.4.1 使用FastAPI完成ChatGLM3私有云交互端口的搭建（重要）
建立ChatGLM3的API接口，可以为其他应用程序提供自然语言处理能力的支持。这对于构建智能化的应用和服务来说，非常重要，可以大大提高开发效率和降低开发成本。同时，API接口也可以促进不同系统和服务之间的信息交互和数据共享，推动自然语言处理技术的发展和应用。
这里使用FastAPI完成接口的创建。FastAPI是一种Python框架，用于构建快速、现代、高效的Web应用程序。它基于Python 3.6及以上版本的类型提示，将Starlette的组件作为其基础构建模块。FastAPI使用标准Python类型提示来定义路由和处理程序，从而提供了清晰、易于理解和易于维护的代码。
FastAPI是一个异步框架，这意味着它支持使用Python的asyncio库来处理高并发生成的大量请求。FastAPI还具有出色的性能，可以与Django和Flask等流行的Python框架相媲美。它还支持依赖注入、中间件和其他现代特性，使得开发人员能够快速构建复杂的应用程序。
下面是一个完整的、基于FastAPI开启服务的示例，代码如下：
```python
import uvicorn
from fastapi import FastAPI, Body
from fastapi.responses import JSONResponse
from typing import Dict
app = FastAPI()
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = "../chatglm3-6b"  # 直接提供ChatGLM3的存储地址
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, 
trust_remote_code=True).half().cuda() # .quantize(4).cuda()
@app.post("/chat")
def f1(data: Dict):
    query = data["query"]
    history = data["history"]
    if history == "":
        history = []
    response, history = model.chat(tokenizer, query, history=history, top_p=0.95, 
temperature=0.95)
    response = {"response": response,"history":history}
    return JSONResponse(content=response)
if __name__ == "__main__":
    uvicorn.run(app, host='127.0.0.1', port=7866)
```
上述代码完成了一个完整的服务应用程序，用于打开本地0.0.0.0地址上的7866端口，接收信息并返回处理结果。
而发送和接收相应的query，则可以通过如下请求发出：
```python
import requests
import json
data = {"query": "你好","history": ""}
encoded_data = json.dumps(data) # 对请求参数进行JSON编码
# 发送POST请求到FastAPI应用程序
response = requests.post("http://127.0.0.1:7866/chat", 
data=encoded_data).json()
# print(response.json()) # 打印响应内容
response_chat = response["response"];history = response["history"]
print(response_chat)
```
可以看到，代码实现了一个简单的发送与接收任务的功能。读者可以自行测试。
# 3.4.2 基于streamlit的ChatGLM3自带的网页客户端
除了基于gradio的ChatGLM3网页客户端外，笔者还提供了基于Streamlit的客户端文件，读者可以打开本书配套源码中的“第3章”文件夹查阅web_demo2.py文件的内容。
使用streamlit运行网页端的方法也很简单，读者只需要打开终端，定位到“第3章”文件夹，在此目录下运行如下命令即可：
```
streamlit run web_demo2.py
```
这里需要注意，部分读者可能需要在Windows系统的PATH环境变量中显式地注册streamlit的执行地址。
此外，以上的运行服务都是通过接口完成的，因此在此方法运行结束后，读者需要手动关闭程序，以便空出接口供后面的学习使用。
# 3.5 本章小结
本章介绍了ChatGLM3的部署和使用。首先，深入探讨了如何使用gradio库将机器学习模型部署为交互式网页应用。gradio是一个强大的库，能够让研究者、开发者迅速地将模型转换为可分享和使用的应用。然后，基于gradio展开了猫狗图像分类的实战。最后，介绍了基于网页端的ChatGLM3部署和使用以及基于私有云服务的ChatGLM3部署和使用。
通过本章的学习，读者不仅应该能够熟练地使用gradio来部署机器学习模型，还能够理解如何将模型部署在私有云服务上，并通过网页端与模型进行交互。这些技能为读者未来在机器学习领域的研究和开发工作提供了实践经验和技术支持。 

