### 第4章 使用ChatGLM3与LangChain实现知识图谱抽取和智能问答
LangChain是一个在应用程序中使用大语言模型的编程框架。它是一系列精心设计的工具、组件和接口的集合，旨在简化由大语言模型和聊天模型提供支持的应用程序的构建过程。LangChain不仅将烦琐的底层细节抽象化，还优雅地处理了与语言模型的复杂交互，使得开发人员能够专注于构建出色的应用程序逻辑。

在LangChain的助力下，开发人员可以轻松地将多个组件链接在一起，如同编织一张精密的网，每个组件都各司其职，协同工作。此外，LangChain还提供了强大的集成能力，允许无缝地接入额外的资源，如API和数据库，进一步丰富了应用程序的功能和实用性。

中文大语言模型中的翘楚——ChatGLM3，不仅拥有庞大的参数规模，还经过了海量的中文数据训练，深谙中文的精髓和韵味。它能够理解复杂的语义，生成流畅自然的文本，为用户提供卓越的交互体验。

从本章开始，我们将开启一段全新的探索之旅，将ChatGLM3与LangChain紧密结合，以期在简化应用开发的流程上迈出坚实的一步。我们将深入挖掘这两大技术的潜力，让它们在相互的激荡中迸发出前所未有的火花。通过LangChain这一强大框架，我们将能够最大程度地开发和提升ChatGLM3的现有能力，释放其蕴藏的无限可能。

在这段旅程中，我们将见证ChatGLM3如何在LangChain的助力下，如同插上了腾飞的翅膀，变得更加高效、智能和灵活。我们将探索如何优雅地管理语言模型的交互，如何将多个组件巧妙地链接在一起，如何无缝地集成额外的资源，从而创建出一款令人瞩目的应用程序。

在这个过程中，我们不仅会挖掘技术的深度，也会追求应用的广度，致力于将ChatGLM3与LangChain结合应用到更多的场景中。无论是智能客服、教育辅导、内容创作还是其他领域，我们都将努力探索其可能的应用边界，为这个世界带来更多的惊喜和改变。

让我们携手踏上这段充满挑战与机遇的探索之旅吧！相信在不久的将来，我们将共同见证ChatGLM3与LangChain结合所带来的丰富的应用场景和变革效果。

#### 4.1 当ChatGLM3遇见LangChain
LangChain框架犹如LLM应用架构中的一颗璀璨明珠，不仅为整个架构注入了活力，更赋予了其无尽的可能性。那么，究竟什么是LLM应用架构呢？简而言之，它是基于语言模型的应用程序设计与开发的根本骨架，是构建智能化应用的坚实基础。

LangChain的魅力在于其强大的整合能力，它巧妙地将LLM模型、向量数据库、交互层的Prompt外部知识以及外部工具融为一体。这种完美的融合，使得开发者能够随心所欲地构建出各种LLM应用，无论是智能对话、文本生成，还是情感分析、知识推理，都能通过LangChain的魔法之手轻松实现。LangChain图标如图4-1所示。

ChatGLM3作为最强的中文大模型，无疑是LangChain最得力的助手。它为LangChain提供了最强大的语言核心，使得整个框架在处理复杂的中文语境时更加游刃有余。ChatGLM3的加入，不仅提升了LangChain的语言理解能力，还带来了更加丰富的语义表达和更精准的文本生成能力。两者相辅相成，共同构建了一个更加强大、智能的LLM应用生态。

##### 4.1.1 LangChain的基本构成、组件与典型场景
在人工智能的持续演进中，语言模型，尤其是大语言模型，例如备受瞩目的ChatGPT，已经稳固地占据了科技前沿的核心地位。这些模型不仅引发了科技界的热烈讨论，更在实际应用中获得了广泛的认同和赞誉。

正是在这样的技术浪潮下，LangChain框架应运而生。作为一个以LLM为基石的开发框架，LangChain赋予了自然语言处理领域前所未有的活力和创造力。通过LangChain，开发者们可以轻松构建出多样化的应用程序，其中聊天机器人和智能问答工具仅仅是冰山一角。这些应用程序不仅能够理解复杂的语言结构，还能生成流畅自然的文本响应，从而为用户带来更加丰富和智能的交互体验。

LangChain的强大之处在于其高度的灵活性和可扩展性。它能够将各种数据源、外部工具和模型整合到一个统一的框架中，使得开发者能够根据具体需求定制和优化应用程序。这种灵活性意味着LangChain不仅能够满足当前的需求，还能够适应未来技术的发展和创新。

具体来看，LangChain主要由两部分组成：LLM终端与LLM编程框架。
- **LLM终端**：LLM终端作为一个稳健的接口，负责管理模型资源并确保可扩展性和容错性。它提供了接收输入并生成输出的接口，使得下游应用程序能够轻松集成LLM功能，并利用其强大的语言处理能力。
- **LLM编程框架**：LLM编程框架提供了一套完整的工具和抽象，用于构建基于语言模型的应用程序。这些框架能够协调各种组件，包括LLM提供商、嵌入模型、向量存储、文档加载器以及外部工具（如在线搜索等），从而简化了应用程序的开发和部署过程。

从具体使用上来看，LangChain由多个组件搭建在一起完成，这些组件共同搭建了服务于大模型核心的LangChain架构。
- **Prompts（提示）**：提示是管理LLM输入的关键工具。为了获得所需的输出，需要对提示进行精细调整。它们可以是一个简单的句子，也可以是由多个句子组成的复杂结构，包含变量、条件语句等元素。
- **Chains（链）**：链是一种强大的工具，能够将LLM与其他组件（如外部API、数据库等）连接起来，以实现更复杂的任务。通过链，可以将多个操作组合成一个连贯的工作流程，从而提高任务执行的效率和准确性。 
- **Agents（代理）**：代理是使用LLM进行决策的工具。它们可以执行特定的任务，并生成文本输出。代理通常由动作、观察和决策三个部分组成，通过这三个部分的协作，代理能够基于当前环境和任务需求做出智能决策。 
- **Memory（记忆）**：由于LLM本身没有长期记忆能力，因此记忆工具被用来在多次调用之间保持状态。这些工具可以存储先前的输入、输出或其他相关信息，以便在后续处理中提供上下文或历史数据。 

在具体应用上，LangChain可以帮助用户针对多个应用场景和项目实际需求提供多种多样的协助支持，从而帮助用户更方便地完成项目的落地。一般在使用上有如下典型应用场景：
- **特定文档的问答**：利用大语言模型技术栈，可以从Notion等数据库中提取特定文档的信息，并准确回答用户的问题。这在企业知识管理、客户服务等领域具有广泛应用前景。
- **聊天机器人**：使用Chat-LangChain模块等工具，可以创建与用户进行自然交流的聊天机器人。这些机器人能够理解用户的意图和需求，并提供有用的回复和建议。
- **代理执行任务**：结合GPT和WolframAlpha等工具，可以创建能够执行数学计算、查询知识库和其他任务的代理。这些代理可以在教育、科研、智能家居等领域发挥重要作用。 
- **文本摘要**：利用外部数据源和LLM技术栈，可以生成特定文档的摘要。这对于快速浏览大量信息、撰写报告或新闻报道等场景非常有用。 

随着LangChain框架的不断迭代和优化，其功能日趋完善，支持的应用场景也愈发广泛。在处理复杂的语言模型任务，乃至解决各类实际问题时，LangChain都展现出了卓越的性能和灵活性。LangChain与ChatGLM3的结合，将为我们提供一个强大的中文大语言模型开发环境。ChatGLM3作为目前领先的中文大模型，拥有出色的语言理解和生成能力，而LangChain则提供了一个灵活、高效的开发框架。二者的结合，将使我们能够更快速、更便捷地构建出高质量的中文自然语言处理应用。

在接下来的内容中，我们将逐步介绍如何将LangChain与ChatGLM3进行集成。首先，将介绍如何在LangChain中引入ChatGLM3模型，并配置相关的参数和设置。接着，将探讨如何利用LangChain的框架特性，对ChatGLM3进行微调和优化，以提升模型在特定任务上的性能。此外，还将介绍如何使用LangChain提供的工具和接口，实现与ChatGLM3的交互和通信，从而构建出功能强大的自然语言处理应用。

##### 4.1.2 确认统一地址的ChatGLM3部署方案
在3.4.1节中，介绍了使用FastAPI完成的ChatGLM3部署方案，通过简单的几行代码即可完成一个ChatGLM3服务器的架设。在架设服务器之前，有一个非常重要的内容需要读者了解，就是需要一个统一的源。如果读者使用单机，则URL设置如下：
```python
url = '127.0.0.1'
```
这是在采用单机模式进行模型开发的情况下对IP地址的设置。如果读者需要在互联网上架设对应的服务器，或者在内部网的服务器上架设，则可以根据不同的场景设置不同的URL地址。
```python
url = '你的内网地址'
```
完整代码如下：
```python
import uvicorn
from fastapi import FastAPI, Body
from fastapi.responses import JSONResponse
from typing import Dict
app = FastAPI()
from modelscope import AutoTokenizer, AutoModel, snapshot_download
model_dir = "../chatglm3-6b"  # 直接提供ChatGLM3的存储地址
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, 
trust_remote_code=True).half().cuda() # .quantize(4).cuda()
@app.post("/chat")
def f1(data: Dict):
    query = data["query"]
    history = data["history"]
    if history == "":
        history = []
    response, history = model.chat(tokenizer, query, history=history, top_p=0.95, 
temperature=0.95)
    response = {"response": response,"history":history}
    return JSONResponse(content=response)
if __name__ == "__main__":
    uvicorn.run(app, host='你预设的地址', port=7866)
```
这里需要强调一下，代码中的地址与我们学习LangChain的过程息息相关，因此建议读者统一这个地址并保存使用。

##### 4.1.3 使用ChatGLM3构建LangChain的LLM终端
LLM是LangChain的基本组成部分，作用是对输入的文本或者向量内容进行处理，将文本字符串作为输入并返回文本字符串的模型。LangChain本身不提供LLM，而是提供通用的接口来访问LLM，我们可以很方便地更换底层的LLM以及自定义LLM。

下面就是自定义的、可供LangChain使用的LLM架构，读者可以直接启动，也可以参考上一章云上架设服务器的方式，完成自定义的LLM服务器架设。代码如下：
```python
import time
import logging
import requests
from typing import Optional, List, Dict, Mapping, Any
import langchain
import torch
from langchain.llms.base import LLM
from langchain.cache import InMemoryCache
logging.basicConfig(level=logging.INFO)
# 启动LLM的缓存
langchain.llm_cache = InMemoryCache()

class ChatGLM(LLM):
    # 模型服务URL，这里地址一定要与开启GLM服务的URL地址相同
    # url = "http://127.0.0.1:7866/chat"  # 本机地址
    url = "http://192.168.3.20:7866/chat"  # 内网其他机器上的地址
    history = []

    @property
    def _llm_type(self) -> str:
        return "chatglm"

    def _construct_query(self, prompt: str) -> Dict:
        """构造请求体"""
        query = {
            "human_input": prompt
        }
        query = {"query": prompt, "history": self.history}
        import json
        query = json.dumps(query)  # 对请求参数进行JSON编码
        return query

    @classmethod
    def _post(self, url: str, query: Dict) -> Any:
        """POST请求"""
        response = requests.post(url, data=query).json()
        return response

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """_call"""
        # construct query
        query = self._construct_query(prompt=prompt)
        # post
        response = self._post(url=self.url, query=query)
        response_chat = response["response"]
        self.history = response["history"]
        return response_chat

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _param_dict = {
            "url": self.url
        }
        return _param_dict

if __name__ == "__main__":
    llm = ChatGLM()
    while True:
        human_input = input("Human: ")
        begin_time = time.time() * 1000
        # 请求模型
        response = llm(human_input)
        end_time = time.time() * 1000
        used_time = round(end_time - begin_time, 3)
        logging.info(f"chatGLM process time: {used_time}ms")
        print(f"ChatGLM: {response}")
```
运行结果如图4-2所示。


![image](https://github.com/user-attachments/assets/d85f4791-0b7e-4705-891f-e595a00a8b6a)



ChatGLM: 你好哦！我是人工智能助手ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。

Human: 南京是哪里的省会

ChatGLM: 南京是中国江苏省的省会，位于江苏省东部，长江下游。南京历史悠久，是中国历史文化名城之一，有许多著名的景点。

INFO:root:chatGLM process time: 1217.484ms

Human: 那里有什么好玩的地方

ChatGLM: 南京有很多好玩的地方，以下是一些著名的景点：

1. 南京城墙：南京城墙是中国现存最长、保存最完整的古代城墙之一，可以骑自行车、步行或驾车绕城一圈。

2. 紫禁城：紫禁城是南京的故宫，是中国古代皇帝的宫殿，现在是南京博物馆。

3. 玄武湖：玄武湖是南京最大的城市湖泊，周围有许多公园和景点，如鸡鸣寺、明孝陵等。



这里需要注意，URL地址的设置必须与开启GLM服务的URL地址相同，即：
```python
url = "服务端地址" = "自定义的LLM终端地址"
```
这样才能保证完成结果的获取。请读者自行运行并验证此代码。 
