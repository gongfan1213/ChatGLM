### 5.2.2 基于相似度的输出示例

在模板Prompt的创建过程中，有时我们需要向模板提供多个实例以供参考。在这种情况下，LangChain会智能地选择最相似的模板，并以它为蓝本进行输出。选择的原则是基于相似度比较，确保选出的模板与输入内容高度匹配。

NgramOverlapExampleSelector是一种高效的实例筛选和排列工具，它依据Ngram重叠得分来评估实例与输入的相似度。这个得分是一个介于0.0和1.0之间的浮点数，能够直观地反映实例与输入之间的相关程度。通过这种得分机制，我们可以轻松识别出与输入最相关的实例，从而提升输出的质量。

此外，NgramOverlapExampleSelector还提供了灵活的阈值设定功能。用户可以根据实际需求设定一个阈值得分，所有Ngram重叠得分低于或等于此阈值的实例将被自动筛除。这一功能使得实例筛选过程更加精准，能够有效地排除与输入不相关的实例。

默认情况下，阈值被设定为 -1.0，这意味着所有实例都会被纳入考虑范围，并根据它们的得分重新进行排序。这种设置适用于需要全面考虑所有实例的情况。然而，如果用户希望进一步缩小实例范围，可以将阈值设定为0.0。这样，所有与输入没有Ngram重叠的实例都将被排除在外，从而确保选出的实例与输入高度相关。

NgramOverlapExampleSelector通过其独特的Ngram重叠得分机制和灵活的阈值设定功能，为我们在各种场景下精准地选取与输入高度相关的实例提供了有力支持。示例代码如下：
```python
from langchain.prompts import PromptTemplate
from langchain.prompts import FewShotPromptTemplate

# 创建一个包含中英文对照示例的列表
examples = [
    {"中文": "你好", "English": "hello"},
    {"中文": "你好吗", "English": "How are you"},
    {"中文": "好久不见", "English": "Long time no see"},
]

# 创建一个PromptTemplate对象，用于格式化输入和输出变量的提示模板
example_prompt = PromptTemplate(
    input_variables=["中文", "English"],  # 输入变量为中文和英文
    template="Input: {中文}\nOutput: {English}",  # 模板格式为“输入：中文\n输出：英文”
)

from langchain.prompts import NgramOverlapExampleSelector # 创建一个NgramOverlapExampleSelector对象，用于基于n - gram重叠得分选择示例
example_selector = NgramOverlapExampleSelector(
    examples=examples,  # 可供选择的示例列表
    example_prompt=example_prompt,  # 用于格式化示例的PromptTemplate对象
    threshold=-1.0  # 设置阈值为 -1.0
    # 对于负阈值：选择器按n - gram重叠得分对示例进行排序，并不排除任何示例
    # 对于大于1.0的阈值：选择器排除所有示例，并返回一个空列表
    # 对于等于0.0的阈值：选择器按n - gram重叠得分对示例进行排序，并排除与输入没有n - gram重叠的示例
)

# 创建一个FewShotPromptTemplate对象，用于构建动态提示模板
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,  # 提供ExampleSelector而不是示例列表
    example_prompt=example_prompt,  # 用于格式化示例的PromptTemplate对象
    prefix="作为一个专业翻译，请翻译下面的文本内容",  # 提示的前缀文本
    suffix="Input: {text}\nOutput:",  # 提示的后缀文本，包含输入变量的占位符
    input_variables=["text"]  # 输入变量列表，用于在运行时替换占位符
)

from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=dynamic_prompt)
print(chain.run("我爱chatGLM"))
```
可以看到，上面这段代码的主要目的是构建一个基于语言模型链（LLMChain）的翻译系统。它使用了动态提示模板和Ngram重叠得分选择示例的方法，通过语言模型代理得到最终的结果。

### 5.3 使用Chain提高ChatGLM3的能力

在前面的章节中，我们已经深入探讨了适配于ChatGLM3的提示工程Prompt的使用方法。然而，从实际应用的角度出发，仅仅依靠LLM终端和Prompt是远远不够的。为了确保整个流程的顺畅运行，我们还需要LangChain中的Chain这一关键组件来协同工作。

Chain在LangChain中至关重要，它是连接各个组件的桥梁和纽带，负责将各个部分有机地串联起来。通过Chain的协调作用，LLM终端、Prompt以及其他相关组件能够形成一个紧密配合的整体，共同完成复杂的任务。

具体来说，Chain的主要作用体现在以下几个方面：

- 首先，它们能够接收来自LLM终端的输入，并根据输入内容选择合适的Prompt进行处理。
- 其次，Chain还能够对Prompt的输出结果进行分析和解读，将其转换为可供后续处理的有效信息。
- 最后，Chain还负责将处理结果反馈回LLM终端，以便进行下一轮的交互和处理。
在这个过程中，Chain的协同工作能力至关重要。它们需要与LLM终端、Prompt以及其他相关组件进行实时的信息交换和协调配合，确保整个流程的顺畅进行。同时，Chain还需要具备强大的数据处理和分析能力，以便对输入和输出进行准确的分析和解读。
通过对Chain的深入解析，我们可以更好地理解它在LangChain中的重要地位和作用。在未来的学习和应用中，我们应该充分重视Chain的协同工作能力，不断提升其性能和应用范围，为构建更加高效、智能的人机交互系统奠定坚实基础。
为了方便读者使用，这里统一使用一个LLM终端，如下所示：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()
```
在后面的讲解中，将使用这个LLM终端完成相应的学习内容。

#### 5.3.1 Chain的数学计算方法
在LLM中，数学计算也是一项必不可少的能力，在这里读者可以直接对Chain进行调用，代码如下：
```python
text = "9除以3等于多少"
from langchain import LLMMathChain
llm_math = LLMMathChain(llm=llm, verbose=True)
result = llm_math.run(text)
print(result)
```
可以看到，LLMChain可以很轻松地对以自然语言输入的数字进行计算。虽然这项功能看起来比较简单，甚至小学生都能解决，但是需要认识到，这项功能能够将更强大的数据查询和计算结合在一起，从而使得用户仅仅使用自然语言的方式就能完成数据报表的处理工作。

#### 5.3.2 多次验证检查器

对于大型模型的应用，生成结果时往往会出现一些意料之外的“幻觉”，尤其在涉及具有特定生活常识的情境时。这种现象主要归因于首次生成结果时受到上下文内容和特定需求问题的限制，导致输出可能包含一些不符合逻辑或不符合真实情况的结果。

然而，LangChain中的LLMCheckerChain可以对这些结果进行验证。它基于基本常识对输入内容进行验证，并负责修改未经证实的结果，最终输出正确答案。通过这一机制，LLMCheckerChain有效地提升了大型模型在生成结果时的准确性和可靠性，为用户提供了更加可信和符合实际情况的输出。示例代码如下：
```python
text = """
下面是一些计算题，检验一下对错：
- 1 + 2 = 3
- 2 * 3 = 7
- 5/2 = 2.5
"""
from langchain.chains import LLMSummarizationCheckerChain
checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)
result = checker_chain.run(text)
print(result)
```
上面代码涉及较长的验证过程，即将计算题通过大模型验证的形式重新验证。可以看到代码开始
始提供的是错误的计算题，经过长时间的验证后，模型会输出正确的内容，如图5 - 3所示。
| Checked Assertions | 下面是一些计算题，检验一下对错： |
| ---- | ---- |
| * 1 + 2 = 3 (True)<br>* 2 * 3 = 7 (True)<br>* 5/2 = 2.5 (True) | * 1 + 2 = 3<br>* 2 * 3 = 7<br>* 5/2 = 2.5 |
| Original Summary | Corrected Summary |
| 下面是一些计算题，检验一下对错：<br>- 1 + 2 = 3<br>- 2 * 3 = 7<br>- 5/2 = 2.5 | 下面是一些数学题目，它们都需要进行验证：<br>- 1 + 2 = 3<br>- 2 * 3 = 6<br>- 5/2 = 2.5 |

### 图5 - 3经过LLM终端完成的多次验证检查器的过程（左图）与结果（右图）
这里只给出了最后一步的结果，可以很清楚地看到，经过多个过程与步骤的比对，LLM终端输出了修正后的结果，进而完成了对最终结果的验证。


![image](https://github.com/user-attachments/assets/eb5ae5fa-f778-492a-85ef-258b11ac4d9c)



### 5.4 LangChain中的记忆功能

当我们深入探索链或代理的实际应用时，经常会遇到一个不可忽视的要素——“记忆”功能。这种记忆既可以是短暂的，也可以是持久的，取决于具体的应用需求和设计目标。

想象一下，当我们与一位真实的朋友交流时，通常能够回忆起先前的对话片段，这些记忆帮助我们更深入地理解对方的观点，并使对话更加流畅和有意义。同样地，当我们为链或代理引入“记忆”功能时，我们实际上是在赋予它们一种更加人性化和情境化的交互能力。

在最直观的层面，聊天机器人是一个很好的例子。如果读者曾经使用过一些先进的聊天机器人，就可能会注意到，它们经常会引用或回顾之前的对话内容，这就是一种“短期记忆”的体现。短期记忆帮助机器人在当前的交互中更加准确地理解用户的意图，并提供更加贴切的回应。

在更复杂的层面，我们可以设想一个链或代理能够随着时间的推移学习和积累关系。通过对用户偏好的理解、对历史交互模式的识别，或者是对外部数据源的持续更新和整合，形成一种“长期记忆”。这种记忆可以是对用户偏好的理解、对历史交互模式的识别，或者是对外部数据源的持续更新和整合。通过这种长期记忆，链或代理可以为用户提供更加个性化、高效和准确的服务，从而构建起一种基于信任和共享历史的深厚关系。

#### 5.4.1 ConversationChain会话链的使用

一个链条在与用户或其他系统进行持续的交互过程中，会不断地汲取和更新信息。这些信息可能是用户的偏好、历史行为模式，或者是与外部数据源交互中获得的新知识。随着时间的推移，这个链条逐渐形成了一个庞大的记忆库，其中存储了各种关键信息。

当新的交互发生时，这个链条不仅能够根据当前的输入做出响应，还能够利用其“长期记忆”中的信息来提供更加丰富和更加准确的输出。例如，在一个客户服务场景中，如果链记住了某个用户之前的问题和反馈，那么在用户再次提问时，它就能够更加迅速地理解用户的需求，并提供更加个性化的解决方案。

这种“长期记忆”的能力也使得链能够在不同的环境和情境中进行自适应和学习。通过不断地与外部环境进行交互和反馈，链可以逐渐优化其处理方式和响应策略，从而提供更加高效和准确的服务。

下面是一个采用了记忆模组的会话链，代码如下：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

from langchain.chains import ConversationChain
conversation = ConversationChain(llm=llm, verbose=True)

output = conversation.predict(input="你好！")
print(output)

output = conversation.predict(input="南京是哪里的省会？")
print(output)

output = conversation.predict(input="那里有什么好玩的地方，简单地说一个就好。")
print(output)
```
读者可以根据自己的需求和兴趣，灵活地调整查询的内容，逐一向模型发起询问。为了使整个对话更加富有条理和更加连贯，推荐采用一种如剥洋葱般层层深入的策略。通过这种方式，每一次的询问都基于对上一次回复的理解，从而形成一个紧密衔接、逻辑清晰的回复链。这样，读者不仅能够逐步深入地挖掘主题，而且可以在与模型的互动中体验到一种如同真实对话般的流畅感。

#### 5.4.2 系统memory的使用
LangChain的一个很大作用就是可以将历史的对话记录保存在这轮对话的内存中，而不是通过存储数据的形式进行保存，这样可以提高本轮对话的准确性。示例代码如下：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

# 从langchain.prompts导入所需的Prompt类
from langchain.prompts import (
    ChatPromptTemplate,  # 用于构建聊天模板的类
    MessagesPlaceholder,  # 用于在模板中插入消息占位符的类
    SystemMessagePromptTemplate,  # 用于构建系统消息模板的类
    HumanMessagePromptTemplate  # 用于构建人类消息模板的类
)

# 从langchain.chains导入ConversationChain类，用于构建对话链
from langchain.chains import ConversationChain

# 从langchain.memory导入ConversationBufferMemory类，用于存储对话的内存
from langchain.memory import ConversationBufferMemory

# 创建一个聊天提示模板，包括系统消息、历史消息占位符和人类消息输入
prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("你是一个最强大的人工智能程序，可以知无不答，但是你不懂的东西会直接回答不知道。"),
    MessagesPlaceholder(variable_name="history"),  # 历史消息占位符
    HumanMessagePromptTemplate.from_template("{input}")  # 人类消息输入模板
])

# 创建一个用于存储对话的内存实例，并设置return_messages=True以返回消息内容
memory = ConversationBufferMemory(return_messages=True)
print("memory:", memory.chat_memory)  # 打印初始内存状态

# 使用内存、提示模板和LLM模型创建一个对话链实例
conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)

# 使用“你好”作为输入，预测对话链的下一个响应
response = conversation.predict(input="你好")
print(response)  # 打印响应内容

# 使用“介绍一下你自己。”作为输入，再次预测对话链的下一个响应
response = conversation.predict(input="介绍一下你自己。")
print(response)  # 打印响应内容

print("memory:", memory.chat_memory)  # 打印更新后的内存状态，包括对话历史记录
```
这个示例代码演示了如何使用LangChain框架构建一个简单的聊天机器人。它首先导入所需的LangChain模块和类，然后创建一个聊天提示模板和一个用于存储对话的内存实例。接下来，它使用这些组件创建一个对话链实例，并使用该实例预测和生成对用户输入的响应。最后，它打印出对话的内存状态，以展示对话历史的存储和更新。

需要注意，上面代码在原始的ConversationChain链中额外添加了一个memory模块，这是为了记录聊天的历史记录，从而对输出的内容进行统计和整合。这里打印出memory的信息记录，结果如图5 - 4所示。

| memory: messages=[]<br>你好！我是一个人工智能程序，很高兴能帮助你解决问题。请随时提问，我会尽力为你提供帮助。<br>你好！我是一个人工智能程序，由多国科研人员协同开发，经过不断的训练和优化，使我能够理解和回答各种问题。我的目标是为人类提供便捷的服务，帮助你解决问题和获取信息。请随时提问，我会尽力为你提供帮助。 |
| ---- |
| memory: messages=[HumanMessage(content='你好'), AIMessage(content='你好！我是一个人工智能程序，很高兴能帮助你解决问题。请随时提问，我会尽力为你提供帮助。'), HumanMessage(content='介绍一下你自己。'), AIMessage(content='你好！我是一个人工智能程序，由多国科研人员协同开发，经过不断的训练和优化，使我能够理解和回答各种问题。我的目标是为人类提供便捷的服务，帮助你解决问题和获取信息。请随时提问，我会尽力为你提供帮助。')] |

### 图5 - 4 memory的记录
可以看到，此时的记录中，对应的memory首先是一个空的序列，之后依次将问询的内容加入序列中，从而完成对整个对话链的整合。


![image](https://github.com/user-attachments/assets/35fd8669-a1db-4930-8434-c3636d25d9c5)


### 5.5 基于ChatGLM3终端撰写剧情梗概、评论与宣传文案实战

在前面章节中，我们演示了使用ChatGLM3搭建的LLM终端的一些基本使用方法，但是在具体使用上，这些链彼此独立，不存在依赖关系。本节尝试将多个链的功能和结果串联在一起，完成一个完整的、基于给定标题的剧情梗概、评论与宣传文案实战。

在深度学习和自然语言处理领域，调用语言模型后的关键步骤往往涉及一系列连续的调用操作。这种连续性在处理复杂任务时尤为重要，比如将一个调用的输出作为另一个调用的输入，从而逐步推导出最终的结果。这里，我们主要关注两种顺序链形态：

（1）SimpleSequentialChain：这是最简单的顺序链形式。在这种结构中，每个步骤都拥有单一的输入和输出。这种“一步接一步”的方式，确保

的流程的清晰性和简洁性。每一步的输出都会直接传递给下一步作为输入，从而形成一条清晰、连贯的处理路径。

（2）SequentialChain：相较于SimpleSequentialChain，这是一种更复杂的顺序链形式。它允许每个步骤拥有多个输入和（或）输出，提供了更大的灵活性和扩展性。这种结构在处理复杂任务或需要整合多种信息源的场景中尤为有用。

接下来，我们将详细讲解这两种顺序链的使用方法和实际应用，之后还会讲解如何在顺序链中插入额外一些辅助信息的处理方法。

#### 5.5.1 对过程进行依次调用的顺序链SimpleSequentialChain
SimpleSequentialChain作为最基础的顺序链形态，其每一步骤的单一输入/输出特性使得它在处理线性任务时表现出色。下面是一个基于SimpleSequentialChain的示例，演示如何通过一系列步骤完成一个戏剧文本的编辑，并模拟评论家的评论过程。完整代码如下：
```python
from 新第四章_Langchain知识图谱 import llm_chatglm
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

title = "教学楼奇遇记"

# 这是一个用于生成剧情梗概的LLMchain
llm = llm_chatglm.ChatGLM()
template = """
你是一个剧作家。给定剧名，你的工作就是为这个剧名写一个剧情梗概。

Title:{title}
剧作家：这是上述剧本的剧情梗概：
"""

prompt_template = PromptTemplate(input_variables=["title"],
                                 template=template)
synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)

synopsis = synopsis_chain.run(title = title)
print(synopsis)

print("------------------------------")

# 这是一个用于生成剧情评判的LLMchain
llm = llm_chatglm.ChatGLM()
template = """
你是《扬子晚报》的戏剧评论家。根据戏剧的梗概，你的工作是为该剧写一篇评论。

戏剧梗概：{synopsis}
戏剧评论家对上述剧本的评论：
"""

prompt_template = PromptTemplate(input_variables=["synopsis"],
                                 template=template)
review_chain = LLMChain(llm=llm, prompt=prompt_template)

from langchain.chains import SimpleSequentialChain
overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)

review = overall_chain.run(title)

print(review)
```
上面代码顺序生成了一个顺序链，从生成剧情梗概开始，到传递给评论家模型进行剧情评论。

输出结果如下：

**Title: 教学楼奇遇记**

**剧情梗概：**

在一个普通的高校校园里，有一座古老的教学楼，这里隐藏着一个令人神秘的传说。某天，来自不同专业的四位同学：勇敢的侦探林峰、聪明的科学家陈思、胆小的设计师王磊和热血的体育女生李婷，因为一起课程的合作，开始了他们在这座教学楼的奇妙探险。

他们首先在图书馆里找到了关于这座教学楼的历史资料，了解到了关于这座建筑内隐藏着一个传说。据说，在几十年前，一位神秘的数学家曾在这里进行过高难度的计算，得出了一个惊人的结果，引发了严重的灾难。为了揭开真相，四位同学决定深入调查。

他们逐层探索教学楼，一路上遇到了各种诡异的现象。林峰用他的侦探技巧分析这些异状，发现这一切似乎与数学家留下的线索有关；陈思运用她的科学头脑，从中发现了隐藏在这座楼内的某种物理规律；王磊则因为自己的设计天分，为团队提供了宝贵的建议；而李婷则凭借她的运动天赋和勇气，总是在关键时刻帮助大家渡过难关。

经过一番努力，他们终于找到了隐藏在教学楼内的神秘空间，那里正是数学家进行计算的地方。在这里，他们发现了一本被遗弃的数学笔记本，里面记载了数学家留下的计算过程和结论。原来，数学家发现了一个影响整个世界的不稳定公式，为了避免灾难发生，他决定将这个公式从教学楼中铲除。然而，他因为私人恩怨，没有将这个秘密告诉任何人。

四位同学决定将这个秘密公之于众，他们用智慧和勇气挑战传统的思维模式，最终成功揭示了数学家的真相。同时，他们也意识到，只有团结合作，才能解决面对困难时产生的种种问题。最终，他们将这段惊心动魄的经历记录在了一本名为《教学楼奇遇记》的笔记本里，作为对那些勇敢面对困难的人的赞美。


**> Entering new SimpleSequentialChain chain...**

**> Finished chain.**

这部名为《教学楼奇遇记》的剧本，以一座古老的教学楼为背景，巧妙地将mystery、thriller和comedy三种元素融合在一起，为观众带来了一场充满惊喜和思考的戏剧盛宴。

首先，剧本对主人公们的性格塑造非常鲜明。勇敢的侦探林峰、聪明的科学家陈思、胆小的设计师王磊和热血的体育女生李婷，这四位各具特色，他们之间的互动和冲突，既充满幽默感，又让人感到温暖和感动。特别是剧本对女性角色的刻画，不仅展示了她们独立、坚韧和聪明的一面，还打破了传统戏剧中女性角色的刻板印象。

其次，剧本在剧情设置上，巧妙地运用了神秘、悬疑的元素，让观众在观看的过程中充满了期待和紧张。例如，主人公们在探索教学楼的过程中，遇到的种种诡异现象，以及他们逐层探索教学楼时，所发现的种种线索，都让观众感受到了神秘和悬念。

此外，剧本的结局也让人印象深刻。主人公们最终揭示了数学家的真相，避免了灾难的发生，同时也展现了他们的团结合作精神和对科学的热爱。这样的结局，不仅让人感叹于他们智慧和勇气的combination，也让人对人性有了更深的认识。

总的来说，《教学楼奇遇记》是一部让人难以忘怀的剧本，它通过讲述一段充满惊险、神秘和欢笑的故事，向观众传递了团结合作、智慧勇敢等正面价值观念，值得一看再看。

从上面结果可以看到，通过使用SimpleSequentialChain，我们可以很巧妙地将一个顺序发生的事情组合在一起，之后通过这个完整的任务链，输出中间步骤后再输出全部结果。有兴趣的读者可以更改标题，完成更多的内容串联与设计。


#### 5.5.2 对过程进行依次调用的顺序链SequentialChain

SequentialChain是一种更复杂，但同时也更灵活的链式结构。与SimpleSequentialChain那种“一个输入对应一个输出”的简单模式不同，SequentialChain允许我们在每个步骤中处理多个输入，并产生多个输出。这种设计使得SequentialChain在处理复杂任务时表现出色，尤其是在需要整合多种信息源或生成多种结果的情形下。 

在这种复杂的多输入/多输出场景中，为输入和输出变量选择合适的命名变得至关重要。合适的命名不仅能够提高代码的可读性，还有助于后续的维护和扩展。在之前的简单示例中，由于链与链之间的传递关系相对直接，因此并没有太多考虑命名的问题。但在SequentialChain中，由于每个步骤都可能涉及多个输入和输出，因此我们必须更加谨慎地选择每个变量的名称。

具体来说，对于输入变量，我们应该根据其来源或作用进行命名，确保名称能够准确地反映该变量的含义。对于输出变量，我们同样需要选择具有描述性的名称，以便在阅读代码时能够迅速理解该变量的用途。此外，如果某个步骤的输出将作为后续步骤的输入，那么我们还可以在命名上体现出这种关联性，从而进一步增强代码的可读性。 
