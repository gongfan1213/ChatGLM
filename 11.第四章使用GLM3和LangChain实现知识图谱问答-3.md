### 86 | ChatGLM3大模型本地化部署、应用开发与微调
#### 4.2.2 数据准备与基础算法分析
由于本项目需要完成专业领域的专业问答，因此笔者准备了一份真实医疗问答实例作为数据基础内容。这个实例是基于具有实际意义的医学问答的病例设计出来的相关医疗常识，内容如图4-7所示。

[此处为图4-7真实的医疗问答数据内容，未完整展示]

![image](https://github.com/user-attachments/assets/90fa4356-5a30-4d6f-ad38-cfed773af18f)


接下来需要对数据进行处理，由于读取的文档内容是以JSON形式存储的，因此读取此内容的代码如下：
```python
import json
# 打开文件，r是读取，encoding是指定编码格式
with open('./dataset/train1.json', 'r', encoding='utf-8') as fp:
    # load()函数将fp(一个支持.read()的文件类对象，包含一个JSON文档)反序列化为一个Python对象
    data = json.load(fp)
    for line in data:
        line = line["context_text"]  # 获取文档中的context_text内容
        context_list.append(line)  # 将获取到的文档添加到对应的list列表中
```
这里需要注意，本例中是将医疗问答数据作为特定的文档目标，读者也可以选择自己的专业领域文档或者内容作为特定目标进行处理。

![image](https://github.com/user-attachments/assets/b1ef5968-6652-42f2-b72f-f1fb1ed1c4b5)


在完成数据的准备工作后，我们可以直接将全部数据发送给LLM终端，让它帮我们查找最合适的答案。然而这样做似乎不太容易，最简单的一个原因就是，当我们一次性发送的问题过多时，LLM终端会因一次性接收过多的内容而影响结果的输出。这个问题的解决方法就是：利用特定方法或算法，从中找出与所提问题最接近的答案。这样，我们的实战任务就转换为了文本相关性（或相似度）的比较与计算问题。

对于文本相关性的计算，读者应该已经有所了解。在实际应用中，余弦相似性计算和BM25相关性计算是两种常用的方法。在本节中，将采用BM25算法来计算文本的相关性。

假如我们有一系列的文档Doc，现在要查询问题Query。BM25的思想是，对Query进行语素解析，生成语素Q；然后对每个搜索文档Di计算每个语素Qi与文档Dj的相关性，最后将所有的语素Qi与Dj进行加权求和，从而最终计算出Query与Dj的相似性得分。BM25算法总结如下：
\[ Score(Query, D)=\sum_{i}^{n}W_{i}\cdot R(Q_{i}, D_{j}) \]
在中文中，我们通常将每一个词语当作Qi，Wi表示语素Qi的权重，R(Qi, Dj)表示语素Qi与文档的相关性得分关系。本书不对BM25做深入说明，有兴趣的读者可以自行研究。

接下来，将通过编程实现BM25算法。虽然我们可以自己编写Python代码来实现BM25函数，但考虑到效率和稳定性，还是建议使用现成的Python类库。这是因为这些类库通常由经验丰富的开发者维护，并经过了持续的优化和改进。所以，我们没有必要“重新发明轮子”，直接使用这些成熟的类库即可。

读者可以使用如下代码安装对应的Python类库：
```bash
pip install rank_bm25  # 注意下画线
```
BM25类库比较常用，用于计算单个文本与文本库之间的BM25值。注意，BM25算法在计算时是以单个字（或词）为基础的。因此，在使用BM25进行相关性计算时，需要将文本拆分为字或词的形式。为了方便读者使用，这里提供了一个完整的相关性计算代码示例：
```python
from typing import List
from rank_bm25 import BM25Okapi

def get_top_n_sim_text(query: str, documents: List[str], top_n: int = 3) -> List[str]:
    tokenized_corpus = [list(doc) for doc in documents]  # 将文档拆分为字符列表
    bm25 = BM25Okapi(tokenized_corpus)  # 初始化BM25模型

    tokenized_query = list(query)  # 将查询语句拆分为字符列表
    results = bm25.get_top_n(tokenized_query, tokenized_corpus, n=top_n)  # 获取最相关的n个文档

    results = ["".join(res) for res in results]  # 将字符列表转换回字符串
    return results

# 示例用法
prompt_text = "红色好看还是绿色好看"
context_list = ["今天晚上吃什么", "你家电话多少", "哪个颜色好看", "明天的天气是晴天", "晚上的月亮好美呀"]

sim_results = get_top_n_sim_text(query=prompt_text, documents=context_list, top_n=1)
print(sim_results)
```
最终结果读者请自行打印查看。需要注意的是，对于文本的相似性比较，有很多种方法，而这里采用的BM25则是最简单易用的模型，且其准确率也有一定的保证。但是，对于实际文本相似性计算来说，在方法的具体选用上，需要根据具体的项目和内容要求进行选择。

#### 4.2.3 使用LangChain完成提示语Prompt工程
这里，假设已经找到了对问题的回答最重要的那部分内容，在将它输入LLM终端前，还需要完成一个标准化的提示语，即使用LangChain完成提示语Prompt工程。我们设计的Prompt如下：
```python
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["query", "document"],
    template="你是一名专业的医务工我们，你会认真地根据你所学的知识来回答医学问题，认真回答{query}这个问题，你也可以参考对应的文献材料：{document}",
)
print(prompt)
```
从上面代码中可以看到，我们以专业的口吻设计了一个参考提供的文献材料来回答对应问题的大模型终端。打印的结果如下：
```
input_variables=['query', 'document'] template='你是一名专业的医务工我们，你会认真地根据你所学的知识来回答医学问题，认真回答{query}这个问题，你也可以参考对应的文献材料：{document}'
```

#### 4.2.4 基于ChatGLM3的LLM终端完成专业问答
下面基于LLM完成专业问答，完整代码如下：
```python
import utils
context_list = []
import json
# 打开文件，r是读取，encoding是指定编码格式
with open('./dataset/train1.json', 'r', encoding='utf-8') as fp:
    # load()函数将fp(支持.read()的文件类对象，包含JSON文档)反序列化为Python对象
    data = json.load(fp)
    for line in data:
        line = line["context_text"]
        context_list.append(line)

query = "小孩牙龈肿痛服用什么药"
print("--------------------------------------------------")
print("经过文本查询的结果如下所示。")
sim_results = utils.get_top_n_sim_text(query=query, documents=context_list)
print(sim_results)
print("--------------------------------------------------")

from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["query", "document"],
    template="你是一名专业的医务工我们，你会认真地根据你所学的知识回答医学问题，认真回答{query}这个问题，也可以参考对应的文献材料：{document}",
)

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = (chain.run({"query":query,"document":sim_results}))
print(result)
```
在上面代码中，首先通过精心设计的数据读取函数完成对所需数据准确、高效的读取。随后根据用户提出的查询内容，利用先进的算法找到与之最近似的答案。最后，将这些精心挑选的答案，连同原始查询问题一同输入LLM终端中。通过LLM终端的强大处理能力和深度学习能力，生成了最终的精准答案，如图4-8所示。

[此处为图4-8经过LLM终端生成的答案内容，未完整展示]

从上面结果可以清晰地看到，相对于传统的、仅依赖于原有内容的答案，经过LLM终端优化和处理后的查询结果，展现出更高的质量和贴合度。这得益于LLM终端对答案的进一步整理和提升，使其在回答上更加符合我们所期望的形式和风格。这种融合了深度学习和参考资料的查询问答方式，无疑为用户提供了更便捷、更高效和更准确的信息获取体验。

![image](https://github.com/user-attachments/assets/ad797cce-d074-484a-bfef-f3eda9c6c580)


### 4.3 使用ChatGLM3的LLM终端搭建知识图谱抽取与智能问答
ChatGLM3作为一个功能强大的文本处理大模型终端，它不仅能够依靠自身的记忆来回答问题、查询和解析文本内容，还具备一种独特的能力，那就是根据所掌握的信息自主构建知识图谱。这种能力使得大模型在处理复杂文本时能够更加深入、全面地理解其中的知识和关系。

将ChatGLM3作为LLM终端的一种高级应用，我们可以从文本中抽取出关键信息，进而构建出详尽且结构化的知识图谱。这一过程不仅展现了ChatGLM3在自然语言处理领域的深厚实力，也提供了一种全新的知识组织和表达方式。

在本章中，我们将深入探讨如何利用ChatGLM3和LLM终端的功能来构建知识图谱，并以此为基础实现一项智能问答应用。我们将从知识图谱的基本原理讲起，逐步介绍如何利用大模型从文本中提取实体、关系和属性等信息，进而构建出完整的知识图谱。随后，我们将介绍如何利用这个知识图谱来回答用户的问题，实现智能问答的功能。

通过本节的学习，读者能够掌握构建知识图谱的核心技术和方法，并深入了解如何利用大模型来实现智能问答应用。

#### 4.3.1 基于ChatGLM3的LLM终端完成知识图谱抽取
首先，将进行基于ChatGLM3的知识图谱抽取工作。为了确保这一过程顺利进行，我们需要做好充分的数据准备。在此，准备了一份包含比对关系的财务报表。当然，读者若希望进行更为丰富的测试，也可以自行准备其他领域的数据集。

随后，将利用ChatGLM3的强大功能，从数据集中抽取出关键实体、属性以及它们之间的关系，进而构建出财务报表的知识图谱。本知识图谱抽取示例，将充分展现ChatGLM3在自然语言处理和知识抽取方面的优势。通过构建知识图谱，我们能够更加直观地表示和理解财务报表中的复杂知识和关系，为后续的智能问答等应用奠定坚实基础。

在完成知识图谱抽取后，将进一步探索如何利用这一图谱来实现智能问答等高级功能。这将涉及图谱的查询、推理和应用等多个方面内容，我们将逐一进行深入探讨。

**第一步：获取查询问题的最近似段落**
实现代码如下：
```python
import json
file_name = "工商银行2021年度报告.txt"
with open(file_name, encoding="utf-8") as f:
    financial_report = f.read()
financial_report = financial_report.strip().split("\n")
context = ""
for line in financial_report:
    line = json.loads(line)
    if line["type"] == "text":
        try:
            con = line["inside"]
            context += con
        except:
            pass
document = context[:480]  # 人为缩减了文本长度
```
从上面代码中可以看到，我们人为地缩减了长度，这是因为对于使用LLM终端进行知识图谱抽取，这本身就是一项需要耗费大量显存的工作。读者可以根据自身的硬件资源调整相应的长度。

**第二步：对文本进行知识图谱的抽取**

![image](https://github.com/user-attachments/assets/72f875fe-eae7-4145-b966-22feaa29d7c6)


接下来完成对输入段落的知识图谱抽取，代码如下（有一定的失败概率）：
```python
from langchain.indexes import GraphIndexCreator
from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

index_creator = GraphIndexCreator(llm=llm)
graph = index_creator.from_text(document)
print(graph.get_triples())
```
在上面代码中，我们直接对截取的文本进行知识图谱抽取，打印结果如图4-9所示。

[此处为图4-9基于LLM终端的知识图谱抽取结果内容，未完整展示]

需要注意，对于使用LLM终端对工商银行财务报表（部分内容）的抽取和知识图谱的建立，这里完成了一个“不定”内容的知识图谱抽取，由于其存在一定的失败概率，因此可能需要读者多试几次，并将结果依次保存，从而完成知识图谱的建立。这里为了演示方便，直接使用某次抽取结果来完成下一步的智能问答示例。

#### 4.3.2 基于ChatGLM3的LLM终端完成智能问答
下面演示一下基于LLM终端完成智能问答的实现方法，完整代码如下：
```python
knowledeg_graph = [('中国工商银行股份有限公司（股票代码：601398）', '1984年1月1日', '成立'), ('中国工商银行股份有限公司（股代码：601398）', '2005年10月28日', '改制'), ('中国工商银行股份有限公司（股票代码：601398）', '2006年10月27日', '上市')]

from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["query", "document"],
    template="你是一名专业的财务工我们，你会认真地根据你所学的知识回答问题，认真回答{query}这个问题，也可以参考对应的文献材料：{document}",
)
query = "工商银行哪一年改制的"

from 新第四章_Langchain知识图谱 import llm_chatglm
llm = llm_chatglm.ChatGLM()

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = (chain.run({"query":query,"document":knowledeg_graph}))
print(result)
```
与先前向LLM终端发送大批量文本内容的方式不同，这里采用知识图谱的形式来传递知识内容。通过将精心构建的知识图谱作为输入，LLM终端能够更加精确地聚焦于问题本质，并从更细致的层面为我们提供更细粒度的智能答复。这种方式不仅提高了答复的准确性，还使得答复更具针对性和实用性。

一个基于知识图谱的细粒度文本问答示例如图4-10所示。

![image](https://github.com/user-attachments/assets/9fd07111-3e60-432f-b3ad-926c74daf25f)


[此处为图4-10基于知识图谱的细粒度文本问答内容，未完整展示]

可以看到，由于我们提供了更详尽的和更结构化的知识内容，LLM终端能够更深入地理解问题，并从知识图谱中检索出与问题紧密相关的信息。这使得机器答复更加贴近问题的实际需求，为用户提供了更有价值的答案。

### 4.4 本章小结
在本章中，我们深入探讨了LangChain的基本应用，并通过示例来展示其强大的功能。从最初的Prompt模板创建，到使用模板完成提示工程，我们逐步掌握了LangChain的核心技术。同时，我们还巧妙地利用了ChatGLM3的卓越性能，成功设计了一个LLM终端，从而更高效地获取问题查询的结果。

在智能问答机器人的构建过程中，我们学习到了一种先进的方法：先查找相关信息，再从中筛选最佳结果，并结合问题生成最终答案。这种方法确保了专业回复的准确性和真实性，使得机器人能够更加贴近实际应用场景。

此外，我们还探索了知识图谱在智能问答系统中的应用。借助ChatGLM3出色的文本处理和分析能力，我们成功构建了针对特定文本的知识图谱。与传统的文本参考资料相比，将知识图谱作为参考资料发送给LLM终端，可以让它为我们提供更细粒度的回答。这种处理方法在需要精确、贴切的结果时显得尤为重要，它为我们提供了一种全新的、高效的智能问答解决方案。 
